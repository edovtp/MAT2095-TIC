---
title: "Escobar & West (1995)"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "dark"
    downcute_theme: "default"
---

```{=html}
<style type="text/css">
.Wrap {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE}
library(tidyverse)
library(collections)
library(here)
library(latex2exp)


## Global options
knitr::opts_chunk$set(cache = TRUE, fig.align = 'center', out.width = "70%")
here::i_am("01_EW_1995/EW_1995.Rmd")
set.seed(219)

## Extra functions
source(here('code', '00_extras.R'))
source(here('code', '01_dirichlet_process.R'))
source(here('code', '02_dirichlet_process_mixtures.R'))

## Algorithms
source(here('01_EW_1995', 'algorithms.R'))
```

This notebook contains replications and implementations of the results given in *Bayesian Density Estimation and Inference Using Mixtures* by Escobar & West (1995).

## TODO:

-   [X] Complete with the results of Antoniak (1974) and Korwar & Hollander

-   [X] Use simplification for the algorithms

-   [X] Implement algorithm 1

    -   [X] Recover parameters

    -   [X] Recover density

-   [X] Implement algorithm 2

    -   [X] Recover parameters

    -   [X] Recover density
    
-   [X] Initial Illustration

-   [X] Implement algorithm 3

------------------------------------------------------------------------

In this paper they consider the following model:

$$
\begin{align*}
  Y_i | \pi_i &\overset{ind.}{\sim} \text{N}(\mu_i, V_i), \quad i = 1, ..., n \\
  \pi_1, ..., \pi_n &\overset{i.i.d.}{\sim} G  \\
  G &\sim DP(\alpha, G_0)
\end{align*}
$$

Where $G_0$ is the Normal-Inverse-Gamma distribution such that (if we replace $G$ with $G_0$),

$$
\begin{align*}
  \mu_j | V_j &\sim \text{N}(m, \tau V_j) \\
  V_j &\sim \Gamma^{-1}(s/2, S/2)
\end{align*}
$$
that is, $G_0 \equiv N-\Gamma^{-1}(m, 1/\tau, s/2, S/2)$.

**Note**: in the paper they mention that $V_j^{-1} \sim G(s/2, S/2)$ with **scale** $S/2$, but it should be the rate.

For use in the following notebook, we define a function named `tic_rdpm_data()` in the file `/code/02_dirichlet_process_mixtures.R` which simulates data from this model returning the values of $\pi$, the value of $k$ (distinct number of components), and the sampled values $y_1, ..., y_n$.

# 1. Antoniak, Korwar & Hollander

In the paper the authors presents the result of Antoniak (1974) which states that the prior for $k$, the number of distinct values among the elements of $\pi$, satisfies that $\text{E}(k|\alpha, n) \approx \alpha \log (1 + n/\alpha)$. On the other hand, Korwar & Hollander (1973), shows that as $n \to infty$ the expected value of $k$ grows as $M \log(n)$. That is,

$$
\text{E}(k|\alpha, n) \approx \alpha \log(1 + n/\alpha) \to \alpha \log(n)
$$

We illustrate both these results in the following figures.

## M = 1

```{r akh m1, echo=FALSE}
n_unique_fun <- function(n, M){
  n <- floor(n)
  sim_data <- tic_rdp_data(n, M, 'rnorm', list(mean = 0, sd = 1))
  n_unique <- length(unique(sim_data))
}

antoniak_fun <- function(n_values, M, plot = TRUE){
  # Vector for the approximate expectations of k
  ek_values <- vector(mode = 'numeric', length = length(n_values))
  
  for (i in seq_along(n_values)){
    k_sim <- replicate(20, n_unique_fun(n_values[i], M), simplify = 'vector')
    ek_values[i] <- mean(k_sim)
  }
  
  if (plot){
    curve(M * log(x), from = 1, to = max(n_values) + 100,
          col = 'purple', lwd = 2)
    curve(M * log(1 + x/M), from = 1, to = max(n_values) + 100,
          col = 'blue', lwd = 2, add = TRUE)
    points(n_values, ek_values, pch = 16, col = 'red')
    legend(x = 'bottomright', legend = c('Korwar & Hollander', 'Antoniak'),
           col = c('purple', 'blue'), lwd = c(2, 2))
  }
  
  return(list(n_values = n_values, ek_values = ek_values))
}

# Values of n
# n_values <- c(100, 500, seq(1000, 21000, by = 2000))

# M = 1
# antoniak_fun(n_values, 1)

knitr::include_graphics(
  here("images", "04-Antoniak_M1.png")
)
```


## M = 50

```{r akh m50, echo=FALSE}
# M = 50
# antoniak_fun(n_values, 50)
knitr::include_graphics(
  here("images", "04-Antoniak_M50.png")
)
```

## M = 100

```{r akh m100, echo=FALSE}
# M = 100
knitr::include_graphics(
  here("images", "04-Antoniak_M100.png")
)
```

## M = 1000

```{r akh m1000, echo=FALSE}
# M = 1000
knitr::include_graphics(
  here("images", "04-Antoniak_M1000.png")
)
```


# 2. Posterior simulation

The main goal here is to obtain the posterior predictive distribution $P(Y_{n+1}|D_n)$ which involves an integral that is extremely computational involved. For this reason, we can use the results for the full conditionals of the posterior distribution to implement a Gibbs Sampler.

The first result stated in the paper is the following: For each $i$, the conditional posterior for $(\pi_i|\pi^{(i)}, D_n)$ is the mixture

$$
\pi_i|\pi^{(i)}, D_n \sim q_0G_i(\pi_i) + \sum_{j\neq i}
q_j \delta_{\pi_j}(\pi_i)
$$

or, equivalently,

$$
\pi_i|\pi^{(i)}, D_n ~ \sim q_0G_i(\pi_i) + \sum_{j=1}^{k_i} q_j^{*}\delta_{\pi_j^*}(\pi_i), \quad q^*_j = n_j q_j
$$

where,

-   $G_i(\pi_i) \equiv N-\Gamma^{-1}(x_i, 1/X; (1 + s)/2, S_i/2)$, that is,
  $$
    \begin{align*}
        \mu_i|V_i &\sim N(x_i, XV_i) \\
        V_i &\sim \Gamma^{-1}((1+s)/2,\, S_i/2)
    \end{align*}
  $$

-   The weights $q_0$ are defined as

$$
    \begin{align*}
    q_0 &\propto \alpha c(s)[1 + (y_i - m)^2/(sM)]^{-(1 + s)/2}/M^{1/2} & \propto \alpha \cdot t_s(m, \sqrt{M})\\
    q_j &\propto \exp[-(y_i - \mu_j)^2/(2V_j)](2V_j)^{-1/2} &\propto N(\mu_j, V_j) \\
    \sum_{j=0, j\neq i}^n q_j &= 1 
    \end{align*}
$$

**Note**: the parameters are

-   $x_i = (m + \tau y_i)/(1 + \tau)$

-   $X = \tau/(1 + \tau)$

-   $S_i = S + (y_i - m)^2/(1 + \tau)$

-   $M = (1 + \tau)S/s$

-   $c(s) = \Gamma((1 + s)/2)\Gamma(s/2)^{-1}s^{-1/2}$

## Algorithm 1

We can use these results to implement a Gibbs sampler with the following algorithm:

1.  Choose starting values for $\pi$, which can be samples from the individual conditional posteriors $G_i$.
2.  Sample elements of $\pi$ sequentially.

The algorithm is implemented in `01_EW_1995/algorithms.R`

### Recovering parameters

To illustrate this algorithm, first we'll use a sample from a DPM model and see if we can recover the true parameters, which include the values of $\pi$ and $k$.

We simulate a sample of size 50 from a Dirichlet Process Mixture Model with parameters $M = 1$, $m = 0$, $\tau = 100$, $s = 50$ and $S = 2$. The reason for this is:

-   For S I just fix the scale parameter of the inverse gamma distribution to 1. Now, a high value of s (relatively to S) gives us both low and less variable values for the variances. This is done so the data drawn from each cluster is close to each other

-   Now, to better illustrate the different clusters, we have to make the means $\mu$ distant from each other. This is done by setting a large value of $\tau$.

We present the sampled values in the following figure.

```{r a1 - rp data, echo=FALSE}
set.seed(219)
n <- 50

a1_data <- tic_rdpm_data(
  n, M = 1,
  m = 0, tau = 100,
  s = 50, S = 2
)

# Plot of the data
plot(density(a1_data$y),
  main = "Simulated data from a Dirichlet Process Mixture",
  xlab = "y"
)
abline(v = unique(a1_data$params[, 1]), col = "red", lwd = 2, lty = "dashed")
```

Next, we run the algorithm using the same priors used to simulate the data.

```{r a1 - rp algorithm (same priors), echo = FALSE}
# alpha is the precision parameter of the DP
prior_params <- list(alpha = 1, m = 0, tau = 100, s = 50, S = 2)
test_a1_rp <- ew_algorithm1(a1_data$y, prior_params, iter = 10000)

# Parameters
sample_means <- colMeans(test_a1_rp, dims = 1)
aux <- cbind(sample_means, a1_data$params)

## Mu
plot(aux[, 3], aux[, 1], main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## V
plot(aux[, 4], aux[, 2], main = latex2exp::TeX(r"(Simulated values of $V$)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

# Number of components
aux <- test_a1_rp[, , 1]
k_sim <- apply(aux, 1, function(x) length(unique(x)))
k_mean <- mean(k_sim)
hist(k_sim, breaks = 1:15, col = 'black', main = 'Sampled values of k')
abline(v = a1_data$k + 0.01, col = "red", lty = 'dashed', lwd = 3)
legend("topright", paste0("real k = ", a1_data$k),
       col = "red", lty = "dashed", lwd = 3)
```

From the figures we can see that it retrieves correctly the mean for the component to the right, and that it tends to clump together the components that are near. On the other hand, the algorithm seems to overestimate the values of $V$.

Finally, we take a sample using different values for the priors.

```{r a1 - rp algorithm (different priors), include=FALSE}
## First: different precision parameter
prior_params <- list(alpha = 10, m = 0, tau = 100, s = 50, S = 2)
test_a1 <- ew_algorithm1(a1_data$y, prior_params, iter = 1000)

# Parameters
sample_means <- colMeans(test_a1, dims = 1)
aux <- cbind(sample_means, a1_data$params)

## Mu
plot(aux[, 3], aux[, 1], main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## V
plot(aux[, 4], aux[, 2], main = latex2exp::TeX(r"(Simulated values of $V$)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

# Number of components
aux <- test_a1[, , 1]
k_sim <- apply(aux, 1, function(x) length(unique(x)))
k_mean <- mean(k_sim)
hist(k_sim, breaks = 1:30, col = 'black', main = 'Sampled values of k')
abline(v = a1_data$k + 0.01, col = "red", lty = 'dashed', lwd = 3)
legend("topright", paste0("real k = ", a1_data$k),
       col = "red", lty = "dashed", lwd = 3)

## Second: different values for G0
prior_params <- list(alpha = 1, m = 3, tau = 10, s = 5, S = 5)
test_a1 <- ew_algorithm1(a1_data$y, prior_params, iter = 1000)

# Parameters
sample_means <- colMeans(test_a1, dims = 1)
aux <- cbind(sample_means, a1_data$params)

## Mu
plot(aux[, 3], aux[, 1], main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## V
plot(aux[, 4], aux[, 2], main = latex2exp::TeX(r"(Simulated values of $V$)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

# Number of components
aux <- test_a1[, , 1]
k_sim <- apply(aux, 1, function(x) length(unique(x)))
k_mean <- mean(k_sim)
hist(k_sim, breaks = 1:10, col = 'black', main = 'Sampled values of k')
abline(v = a1_data$k + 0.01, col = "red", lty = 'dashed', lwd = 3)
legend("topright", paste0("real k = ", a1_data$k),
       col = "red", lty = "dashed", lwd = 3)

## Third: all values different
prior_params <- list(alpha = 5, m = 3, tau = 10, s = 5, S = 5)
test_a1 <- ew_algorithm1(a1_data$y, prior_params, iter = 1000)

# Parameters
sample_means <- colMeans(test_a1, dims = 1)
aux <- cbind(sample_means, a1_data$params)

## Mu
plot(aux[, 3], aux[, 1], main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## V
plot(aux[, 4], aux[, 2], main = latex2exp::TeX(r"(Simulated values of $V$)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

# Number of components
aux <- test_a1[, , 1]
k_sim <- apply(aux, 1, function(x) length(unique(x)))
k_mean <- mean(k_sim)
hist(k_sim, breaks = 1:20, col = 'black', main = 'Sampled values of k')
abline(v = a1_data$k + 0.01, col = "red", lty = 'dashed', lwd = 3)
legend("topright", paste0("real k = ", a1_data$k),
       col = "red", lty = "dashed", lwd = 3)
```

### Recovering density

Now, we simulate data not directly from a DPM, and then try to recover the density. For this, we simulate data from the following model:

$$
y_1, ..., y_n \overset{i.i.d}{\sim} p_1 N(-5, 1) + p_2 N(-1, 1) + p_3 N(0, 1) + p_4N(5, 1)
$$

such that $p_1 + ... + p_4 = 1$. We simulate data from this mixture and then plot the true density in blue.

```{r a1 - rd data, echo=FALSE}
set.seed(219)
n <- 100
probs <- c(0.15, 0.25, 0.3, 0.2)
nmeans <- c(-5, -1, 0, 5)
components <- sample(1:4, prob = probs, size = n, replace = TRUE)
samples <- rnorm(n) + nmeans[components]

true_dens <- function(x){
  probs[1] * dnorm(x, nmeans[1]) + probs[2] * dnorm(x, nmeans[2]) +
    probs[3] * dnorm(x, nmeans[3]) + probs[4] * dnorm(x, nmeans[4])
}

# Samples from the mixture and true density
hist(samples, main = "", breaks = 20, freq = FALSE, col = 'white')
curve(true_dens, add = TRUE, col = 'blue', lwd = 2)
abline(v = nmeans, col = 'red', lty = 'dashed', lwd = 3)
```

Now, we try to see if we can retrieve both the number of components and the real density.

```{r a1 - rd algorithm, echo=FALSE}
# Algorithm
prior_par <- list(alpha = 1, m = 0, tau = 10, s = 4, S = 2)
test_a1_rd <- ew_algorithm1(samples, prior_par, iter = 1000)

## See if we can retrieve the components
plot(nmeans[components], colMeans(test_a1_rd, dims = 1)[, 1], ylim = c(-7, 7),
     main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## See if we can retrive the density
cond_dens <- function(y, pi){
  # Density of y_(n + 1) | pi
  list2env(prior_par, envir = environment())
  n <- nrow(pi)
  
  dens <- alpha * dstudent(
      y, nu = s, mu = m, sigma = sqrt((1 + tau) * S / s)
    ) + sum(dnorm(y, pi[, 1], sqrt(pi[, 2])))
  
  return(dens / (alpha + n))
}

y_grid <- seq(-8, 8, length.out = 500)
dens_est <- vector(mode = "numeric", length = n)

for (j in seq_along(y_grid)){
  dens_est[j] <- mean(
    apply(test_a1_rd, 1, function(x, y){cond_dens(y, x)}, y = y_grid[j])
  )
}

curve(true_dens, col = 'blue', lwd = 2, from = -8, to = 8, ylim = c(0, 0.3),
      main = "Posterior predictive density", ylab = "Density")
lines(y_grid, dens_est, col = "red", lwd = 2)
legend("topright", c("Real", "Estimation"), col = c("blue", "red"),
       lwd = 2)
```

Finally, we repeat the example when trying to recover the real parameters, and check if the density estimate is close to the one that R gives (here we don't know the real density)

```{r a1 - rd for rp, echo=FALSE}
y_grid <- seq(-8, 8, length.out = 500)
dens_est <- vector(mode = "numeric", length = n)
prior_par <- prior_params

for (j in seq_along(y_grid)){
  dens_est[j] <- mean(
    apply(test_a1_rp, 1, function(x, y){cond_dens(y, x)}, y = y_grid[j])
  )
}

plot(density(a1_data$y), col = 'blue', lwd = 2, ylim = c(0, 1),
     main = "Posterior predictive density", ylab = "Density")
lines(y_grid, dens_est, col = "red", lwd = 2)
legend("topright", c("Real", "Estimation"), col = c("blue", "red"),
       lwd = 2)
```

## Algorithm 2

Now we consider an extension of the model to include learning about the prior parameters $m$ and $\tau$. The model is:

$$
\begin{align*}
  Y_i | \pi_i &\overset{ind.}{\sim} \text{N}(\mu_i, V_i), \quad i = 1, ..., n \\
  \pi_1, ..., \pi_n &\overset{i.i.d.}{\sim} G  \\
  G &\sim DP(\alpha, G_0) \\
  G_0 &= N-\Gamma^{-1}(m, 1/\tau, s/2, S/2) \\
  \tau &\sim \Gamma^{-1}(w/2, W/2) \\
  m &\sim N(a, A)
\end{align*}
$$

It follows that

* Given $\tau$ and $\pi$, $m$ is conditionally independent of $D_n$ with distribution

$$
m | \tau, \pi \sim N\Big[(1 - x)a + x\bar{V}\sum(V_j^*)^{-1}\mu_j^*\,;\, x\tau\bar{V}\Big]
$$
where $x = A/(A + \tau \bar{V})$ and $\bar{V}^{-1} = \sum(V_j^*)^{-1}$.

* Given $m$ and $\pi$, $\tau$ is conditionally independent of $D_n$ with distribution

$$
\tau | m, \pi \sim \Gamma^{-1}((w + k)/2, (W + K)/2)
$$
where $K = \sum_{j=1}^k (\mu_j^* - m)^2/V_j^*$. The algorithm is implemented in `01_EW_1995/algorithms.R`.

Now we repeat the same experiments that we did with the first algorithm.

### Recovering Parameters

```{r a2 - rp data, echo=FALSE}
set.seed(219)
n <- 50
real_m   <- 0
real_tau <- 100

a2_data <- tic_rdpm_data(
  n, M = 1,
  m = real_m, tau = real_tau,
  s = 50, S = 2
)

plot(density(a2_data$y),
  main = "Simulated data from a Dirichlet Process Mixture",
  xlab = "y"
)
abline(v = unique(a2_data$params[, 1]), col = "red", lwd = 2, lty = "dashed")
```

```{r a2 - rp algorithm (same priors), echo=FALSE}
# Algorithm
prior_params <- list(alpha = 1, a = 0, A = 100, w = 0.01, W = 0.01, s = 50, S = 2)
test_a2_rp <- ew_algorithm2(a2_data$y, prior_params, iter = 10000)

# Parameters
## m
hist(test_a2_rp$mt[, 1], col = "gray", main = TeX(r"(Sampled values of m)"),
     xlab = "m")
abline(v = real_m, lty = "dashed", col = "red", lwd = 3)

## tau
hist(test_a2_rp$mt[, 2], col = "gray", main = TeX(r"(Sampled values of \tau)"),
     xlab = TeX(r"(\tau)"))
abline(v = real_tau, lty = "dashed", col = "red", lwd = 3)

## mu_j
sample_means <- colMeans(test_a2_rp$pi, dims = 1)
plot(a2_data$params[, 1], sample_means[, 1],
     main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## V_j
plot(a2_data$params[, 2], sample_means[, 2],
     main = latex2exp::TeX(r"(Simulated values of $V$)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = "red")

# Number of components
k_sim <- apply(test_a2_rp$pi[, , 1], 1, function(x) length(unique(x)))
k_mean <- mean(k_sim)
hist(k_sim, breaks = 1:15, col = 'black', main = 'Sampled values of k', xlab = "k")
abline(v = a2_data$k + 0.01, col = "red", lty = 'dashed', lwd = 3)
legend("topright", paste0("real k = ", a2_data$k),
       col = "red", lty = "dashed", lwd = 3)
```

### Recovering Density

```{r a2 - rd data, echo=FALSE}
set.seed(219)
n <- 100
probs <- c(0.15, 0.25, 0.3, 0.2)
nmeans <- c(-5, -1, 0, 5)
components <- sample(1:4, prob = probs, size = n, replace = TRUE)
samples <- rnorm(n) + nmeans[components]

true_dens <- function(x){
  probs[1] * dnorm(x, nmeans[1]) + probs[2] * dnorm(x, nmeans[2]) +
    probs[3] * dnorm(x, nmeans[3]) + probs[4] * dnorm(x, nmeans[4])
}

# Samples from the mixture and true density
hist(samples, main = "", breaks = 20, freq = FALSE, col = 'white')
curve(true_dens, add = TRUE, col = 'blue', lwd = 2)
abline(v = nmeans, col = 'red', lty = 'dashed', lwd = 3)
```

Now, we try to see if we can retrieve both the number of components and the real density.

```{r a2 - rd algorithm, echo=FALSE}
# Algorithm
prior_par <- list(alpha = 1, a = 0, A = 100, w = 0.1, W = 0.1, s = 4, S = 2)
test_a2_rd <- ew_algorithm2(samples, prior_par, iter = 1000)

## See if we can retrieve the components
plot(nmeans[components], colMeans(test_a2_rd$pi, dims = 1)[, 1], ylim = c(-7, 7),
     main = latex2exp::TeX(r"(Simulated values of \mu)"),
     pch = 16, xlab = "Real values", ylab = "Simulated values")
abline(0, 1, col = 'red')

## See if we can retrive the density
cond_dens <- function(y, pi, mt){
  # Density of y_(n + 1) | pi, m, tau
  list2env(prior_par, envir = environment())
  n <- nrow(pi)
  
  dens <- alpha * dstudent(
      y, nu = s, mu = mt[1], sigma = sqrt((1 + mt[2]) * S / s)
    ) + sum(dnorm(y, pi[, 1], sqrt(pi[, 2])))
  
  return(dens / (alpha + n))
}

y_grid <- seq(-8, 8, length.out = 500)
dens_est <- vector(mode = "numeric", length = n)

for (j in seq_along(y_grid)){
  iter <- nrow(test_a2_rd$pi)
  all_dens <- vector(mode = "numeric", length = iter)
  for (i in seq_len(iter)) {
    all_dens[i] <- cond_dens(y_grid[j], test_a2_rd$pi[i, , ], test_a2_rd$mt[i, ])
  }
  
  dens_est[j] <- mean(all_dens)
}

curve(true_dens, col = 'blue', lwd = 2, from = -8, to = 8, ylim = c(0, 0.3),
      main = "Posterior predictive density", ylab = "Density")
lines(y_grid, dens_est, col = "red", lwd = 2)
legend("topright", c("Real", "Estimation"), col = c("blue", "red"),
       lwd = 2)
```

The results are almost the same.

# Initial Illustration (Roeder 1990)

Now we try to reproduce the results presented in the paper about the data given by Roeder 1990.

# Final algorithm

To finalize the article, the authors present a final extension of the previous algorithm that now includes learning about the precision parameter $\alpha$.

This final model is represented as,

$$
\begin{align*}
  Y_i | \pi_i &\overset{ind.}{\sim} \text{N}(\mu_i, V_i), \quad i = 1, ..., n \\
  \pi_1, ..., \pi_n &\overset{i.i.d.}{\sim} G  \\
  G &\sim DP(\alpha, G_0) \\
  G_0 &= N-\Gamma^{-1}(m, 1/\tau, s/2, S/2) \\
  \tau &\sim \Gamma^{-1}(w/2, W/2) \\
  m &\sim N(0, A), \quad A \to \infty \\
  \alpha & \sim \Gamma(a, b) 
\end{align*}
$$

It follows that,

* The full conditional of $\alpha$ is given by,

$$
\alpha | \pi, m, \tau, D_n \equiv \alpha | \eta, k \sim \pi_\eta \Gamma(a + k, b - \log \eta) + (1 - \pi_\eta)\Gamma(a + k - 1, b - \log \eta)
$$

where $\pi_\eta/(1 - \pi_\eta) = (a + k - 1)/[n (b - \log \eta)]$. Here we introduced an auxiliary variable $\eta$ that satisfies

$$
\eta | \alpha, k \sim \text{Beta}(\alpha + 1, n)
$$

* The full conditional of $m$ is given by,

$$
m | \tau, \pi, \alpha, D_n \equiv m | \pi, \tau \sim N\Big[ x\bar{V}\sum(V_j^*)^{-1}\mu_j^*\,;\, x\tau\bar{V}\Big]
$$

  where $x = A/(A + \tau \bar{V})$ and $\bar{V}^{-1} = \sum(V_j^*)^{-1}$.

* The full conditional of $\tau$ is given by,

$$
\tau | \pi, m, \alpha, D_n \equiv \tau | \pi, m \sim \Gamma^{-1}((w + k)/2, (W + K)/2)
$$

  where $K = \sum_{j=1}^k (\mu_j^* - m)^2/V_j^*$.

* The full conditionals of $\pi$ are given by,

$$
\pi_i|\pi^{(i)}, m, \tau, \alpha, D_n \sim q_0G_i(\pi_i) + \sum_{j\neq i}
q_j \delta_{\pi_j}(\pi_i)
$$

  where $G_i(\pi_i) \equiv N-\Gamma^{-1}(x_i, 1/X; (1 + s)/2, S_i/2)$ and 

$$
    \begin{align*}
    q_0 &\propto \alpha c(s)[1 + (y_i - m)^2/(sM)]^{-(1 + s)/2}/M^{1/2} & \propto \alpha \cdot t_s(m, \sqrt{M})\\
    q_j &\propto \exp[-(y_i - \mu_j)^2/(2V_j)](2V_j)^{-1/2} &\propto N(\mu_j, V_j) \\
    \sum_{j=0, j\neq i}^n q_j &= 1 
    \end{align*}
$$

  with

  -   $x_i = (m + \tau y_i)/(1 + \tau)$
  
  -   $X = \tau/(1 + \tau)$
  
  -   $S_i = S + (y_i - m)^2/(1 + \tau)$
  
  -   $M = (1 + \tau)S/s$
  
  -   $c(s) = \Gamma((1 + s)/2)\Gamma(s/2)^{-1}s^{-1/2}$

Thus, the algorithm proceeds as follows:

1. Sample initial values in the following way:
    * Sample $\alpha \sim \Gamma(a, b)$
    * Sample $\tau \sim \Gamma^{-1}(w/2, W/2)$
    * Sample $m \sim N(0, 1)$
    * Sample $\pi$ given $m, \tau$
2. For $t = 1, ..., N$:
    * Sample $\eta_{(t)}|\alpha_{(t-1)}, k_{(t-1)}$
    * Sample $\alpha_{(t)} | \eta_{(t)}, k_{(t-1)}$
    * Sample $m_{(t)} | \pi_{(t-1)}, \tau_{(t-1)}$
    * Sample $\tau_{(t)} | \pi_{(t-1)}, m_{(t)}$
    * Sample $\pi_{i, (t)} | \pi^{(i)}_{(t-1)}, m_{(t)}, \tau_{(t)}, \alpha_{(t)}, D_n$ from its full conditional. Be aware that $\pi^{(i)}_{(t-1)}$ may contain already updated values of the form $\pi_{1, (t)}, ..., \pi_{i-1, (t)}, \pi_{i+1, (t-1)}, ..., \pi_{n, (t-1)}$.

The algorithm is implemented at `01_EW_1995/algorithms.R`.

First, we visualize the data of the galaxy velocities.


```{r galaxies data, echo=FALSE}
velocities <- c(9172, 9558, 10406, 18419, 18927, 19330, 19440, 19541,
                19846, 19914, 19989, 20179, 20221, 20795, 20875, 21492,
                21921, 22209, 22314, 22746, 22914, 23263, 23542, 23711,
                24289, 24990, 26995, 34279, 9350, 9775, 16084, 18552,
                19052, 19343, 19473, 19547, 19856, 19918, 20166, 20196,
                20415, 20821, 20986, 21701, 21960, 22242, 22374, 22747,
                23206, 23484, 23666, 24129, 24366, 25633, 32065, 9483,
                10227, 16170, 18600, 19070, 19349, 19529, 19663, 19863,
                19973, 20175, 20215, 20629, 20846, 21137, 21814, 22185,
                22249, 22495, 22888, 23241, 23538, 23706, 24285, 24717,
                26960, 32789)/1000

hist(velocities, breaks = 1:40)
```

Next, we start the algorithm with the values given in the paper.

```{r algorithm}
# Prior parameters
n <- length(velocities)
prior_par <- list(
  a = 2, b = 4,
  w = 1, W = 100,
  s = 4, S = 2,
  A = 1000
)

# Algorithm
set.seed(219)
N <- 10000
warmup <- 2000

galaxies_inf <- ew_algorithm(velocities, prior_par, iter = N, warmup = warmup)
```

## Posterior predictive density p(y|D)

```{r ppd, echo=FALSE}
## Posterior predictive density P(y|D) (Figure 1)
cond_dens <- function(sample, y){
  # Density of y_(n + 1) | pi, m, tau, alpha, D
  list2env(prior_par, envir = environment())
  
  mu <- sample[1:82]
  V <- sample[83:164]
  m <- sample[165]
  tau <- sample[166]
  alpha <- sample[167]
  
  dens <- alpha * dstudent(
    y, nu = s, mu = m, sigma = sqrt((1 + tau) * S / s)
  ) + sum(dnorm(y, mu, sqrt(V)))
  
  return(dens / (alpha + n))
}

y_grid <- seq(8, 40, length.out = 100)
samples <- cbind(galaxies_inf$pi[, , 1], galaxies_inf$pi[, , 2],
                 galaxies_inf$mt, galaxies_inf$alpha, galaxies_inf$eta)
dens_est <- sapply(y_grid, function(y) mean(apply(samples, 1, cond_dens, y = y)))
hist(velocities, breaks = 1:40, freq = FALSE, 
     main = "Posterior predictive density estimation")
lines(y_grid, dens_est, col = "red", lwd = 2)

## Predictive cdf p(y|D) (Figure 3)
plot(y_grid, cumsum(dens_est), type = "l")
```

## Posterior p(tau|D)

```{r posterior tau, echo=FALSE}
cond_dens <- function(sample, y){
  # Density of tau | pi, m, alpha, D
  list2env(prior_par, envir = environment())
  
  pi <- cbind(sample[1:82], sample[83:164])
  unique_pi <- unique(pi)
  m <- sample[165]
  
  k <- nrow(unique_pi)
  K <- sum((unique_pi[, 1] - m)^2 / unique_pi[, 2])
  
  dens <- dgamma(1/y, (w + k)/2, rate = (W + K)/2) * y^(-2)
  
  return(dens)
}

y_grid <- seq(0.1, 250, length.out = 100)
dens_est <- sapply(y_grid, function(y) mean(apply(samples, 1, cond_dens, y = y)))
plot(y_grid, dens_est, col = "red", lwd = 3, "l", ylim = c(0, 0.008))
curve(dgamma(1/x, prior_par$w/2, rate = prior_par$W/2)*x^(-2), from = 0, to = 250,
      lty = "dashed", lwd = 3, col = "blue", add = TRUE)
legend("topright", legend = c("prior", "posterior"), col = c("blue", "red"), 
       lty = c("dashed", "solid"), lwd = 3)

```

## Posterior p(alpha|D)

```{r posterior alpha, echo=FALSE}
cond_dens <- function(sample, y){
  # Density of alpha | eta, k
  list2env(prior_par, envir = environment())
  
  pi <- cbind(sample[1:82], sample[83:164])
  unique_pi <- unique(pi)
  k <- nrow(unique_pi)
  eta <- sample[168]
  
  odds_w <- (a + k - 1)/(n * (b - log(eta)))
  weight <- odds_w / (1 + odds_w)
  
  dens <- weight * dgamma(y, a + k, rate = b - log(eta)) +
    (1 - weight) * dgamma(y, a + k - 1, rate = b - log(eta))
  
  return(dens)
}

y_grid <- seq(0, 3, length.out = 50)
dens_est <- sapply(y_grid, function(y) mean(apply(samples, 1, cond_dens, y = y)))
plot(y_grid, dens_est, col = "red", lwd = 3, "l", ylim = c(0, 1.6))
curve(dgamma(x, 2, rate = 4), from = 0, to = 3, lty = "dashed", lwd = 3, col = "blue",
      add = TRUE)
legend("topright", legend = c("prior", "posterior"), col = c("blue", "red"), 
       lty = c("dashed", "solid"), lwd = 3)

```

## Posterior p(k|D)

```{r posterior k, echo=FALSE}
k_sim <- apply(galaxies_inf$pi[, , 1], 1, function(x) length(unique(x)))
hist(k_sim, breaks = 1:20, col = 'black', main = 'Sampled values of k', xlab = "k")
table(k_sim)/(N - warmup)
```

We get almost the same results that appear in the paper.

# Notes (updated 28/09)

1. I can't seem to retrieve $k$ correctly.
2. I implemented the first algorithm using the k different values, but now it takes much more time to get a sample.
3. Questions about the number of modes and prior predictive distribution
4. Values for $k$ are inflated, looks like there is a problem with the updating of $\pi$.
5. Mode on the initial illustration is not right
6. On the final algorithm, the posterior full conditional of $\eta$ doesn't depend on $k$