---
number-sections: true
highlight-style: pygments
format: 
  pdf:
    documentclass: article
    toc: true
    include-in-header: header.tex
    include-before-body: title_page.tex
    toc-title: "Tabla de Contenidos"
    toc-depth: 3
  html:
    toc: true
    code-fold: true
execute: 
  cache: true
crossref: 
  fig-title: Figura
  tbl-title: Tabla
bibliography: references.bib
---

\newpage

\section*{Prefacio}

El objetivo de este taller es implementar el algoritmo SIGN [@ni_scalable_2020], que permite aplicar modelos de Mezcla Proceso de Dirichlet (DPM) para bases de datos grandes.

Todo este material fue confeccionado durante mi Taller de Iniciación Científica junto al profesor Fernando Quintana, durante el segundo semestre del 2022. Todo el código y material se encuentra disponible en el siguiente [repositorio](https://github.com/edovtp/MAT2095-TIC) de Github.

Mencionar los paquetes de R disponibles. [@jara_dppackage_2011], [@ross_dirichletprocess_2020]

\newpage
# Introducción

El material presentado a continuación se enmarca en el área de la Estadística Bayesiana No-Paramétrica, por lo que, en primer lugar, sería una buena idea presentar una breve introducción de ambos conceptos.

## Estadística Bayesiana

### Tipos de Incertidumbre

Es común que al presentarnos como estadísticos se nos pregunte acerca de qué es lo que hacemos en nuestro trabajo, ante lo cual solemos responder que nuestro objetivo principal es el de cuantificar la incertidumbre. Explicamos, además, que para lo anterior nos apoyamos sobre la teoría de probabilidades, tomándola como herramienta principal para modelar aquellas incertezas de interés.

Pero, como estadísticos podemos ir un paso atrás y preguntarnos acerca de a qué nos referimos exactamente con incertidumbre. Esta pregunta es la que nos lleva a las bases mismas de la estadística, así como a entender cómo surgen dos visiones que son diferentes entre sí: la **Estadística Frecuentista** (también denominada clásica) y la **Estadística Bayesiana**.

En particular, se distinguen dos tipos de incertidumbre [@ohagan_dicing_2004]. Una de ellas la podemos denominar **incerteza ontólogica** (o aleatoria), mientras que la otra toma el nombre de **incerteza epistemológica** [^1].

[^1]: Es importante mencionar que la ontología es el estudio filosófico del *ser*, mientras que la epistemología es el estudio filosófico del *saber*.

La incerteza ontológica trata acerca de una incerteza que está sujeta a una variabilidad aleatoria innata, que no podemos predecir bajo ninguna cantidad de información. Dentro de los ejemplos de incerteza ontológica se encuentran varios de los ejemplos introductorios a la estadística, como el lanzamiento de un dado o el de ganar la lotería.

Por otro lado, la incerteza epistemológica, tal como lo dice el nombre, es una incerteza acerca de lo que sabemos. La diferencia con la anterior es que en este caso sí podemos obtener información para disminuir, e incluso a veces eliminar, la incerteza. Por ejemplo, podemos tener incerteza acerca de la altura del Costanera Center en Santiago, pero podemos fácilmente buscar en internet esta información, eliminando completamente la incerteza (de hecho, el edificio central tiene una altura de 300 metros).

Así, cuando hablamos de cuantificar la incertidumbre con probabilidades, podemos notar que siempre nos hemos estado refiriendo a incertezas ontológicas. En este sentido, las probabilidades se interpretan como la frecuencia de ocurrencia de un evento, considerando un número infinito de repeticiones. Este es el paradigma frecuentista de la estadística.

Ahora, ¿por qué no podemos modelar también las incertezas epistemológicas mediante probabilidades?. Esto es justamente lo que propone el paradigma Bayesiano. En este caso ya no podemos interpretar las probabilidades como frecuencias, si no que como una *medida racional de incerteza*, lo cual normalmente dependerá de cada persona.

### Modelos Bayesianos

Como vimos, en el paradigma Bayesiano asignamos probabilidades a todas las cantidades desconocidas, sean observables u no observables. Normalmente, en un modelo estadístico estas cantidades no observables son representados mediante parámetros. Esto nos lleva a definir un modelo de probabilidad conjunto, i.e.

$$
p(\mathbf{y}, \theta) = p(\mathbf{y}|\theta)\pi(\theta)
$$

donde $\theta$ no es necesariamente unidimensional. Luego, a la luz de nueva información, se actualiza nuestra creencia a priori, mediante el teorema de Bayes, para obtener entonces la distribución a posteriori que llamaremos simplemente **posteriori** en lo que sigue.

$$
\begin{aligned}
  \pi(\theta|\mathbf{y}) &= \frac{f(\mathbf{y}|\theta)\pi(\theta)}{\int_{\Theta}f(\mathbf{y}|\theta)\pi(\theta)d\theta} \\
  &\propto f(\mathbf{y}|\theta)\pi(\theta)
\end{aligned}
$$ {#eq-posterior}

Por otro lado, es posible que también tengamos interés en la predicción de valores observables a futuro. Esto se obtiene fácilmente marginalizando la incerteza con respecto a las cantidades no observables, i.e.

$$
\begin{aligned}
  p(y_{n+1}|\mathbf{y}) &= \int p(y_{n+1}, \theta|\mathbf{y})d\theta \\
  &=\int p(y_{n+1}|\theta)\pi(\theta|\mathbf{y})d\theta
\end{aligned}
$$ {#eq-posterior-predictive}

donde en la segunda ecuación realizamos el supuesto de independencia condicional entre las observaciones, dado los valores del vector de parámetros $\theta$. Ambos resultados, [-@eq-posterior] y [-@eq-posterior-predictive], podemos considerarlos como los productos principales para ser utilizados en la inferencia estadística mediante este paradigma.

\newpage
```{=tex}
\begin{ejemplo}[Modelo Normal-Normal]
Para aterrizar los conceptos anteriores, consideremos el siguiente modelo:
$$
\begin{aligned}
  y_1, ..., y_n | \theta &\overset{i.i.d.}{\sim} \text{N}(\theta, \sigma^2) \\
  \theta &\sim \text{N}(\mu_0, \sigma_0^2)
\end{aligned}
$$

donde $\sigma^2$ es conocido. Utilizando la ecuación (1) es fácil mostrar que

$$
  \theta | y_1, ..., y_n \sim N(\mu_n, \sigma_n^2)
$$

donde 

$$
  \mu_n = \frac{(1/\sigma_0^2)\mu_0}{1/\sigma_0^2 + n/\sigma^2} + \frac{(n/\sigma^2)\bar{y}}{1/\sigma_0^2 + n/\sigma^2}
$$

y

$$
  \sigma_n^2 = \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}
$$

Por otro lado, también es posible mostrar que

$$
  y_{n+1}|\mathbf{y} \sim \text{N}(\mu_n, \sigma^2 + \sigma_n^2)
$$
\end{ejemplo}
```

Los resultados anteriores muestran una de las características principales de la estadística bayesiana, que es la combinación de la información a priori con la información de los datos para los resultados a posteriori. En particular, notamos que la media a posteriori corresponde a un promedio ponderado de la media a priori y el promedio muestral.

Además, en el ejemplo anterior obtuvimos un resultado bastante conveniente. Al considerar una función de verosimilitud Normal, así como una priori Normal para $\theta$, obtuvimos que la posteriori sigue siendo una distribución Normal. Este tipo de modelos se denominan **conjugados** y serán importantes en los siguientes capítulos. Presentamos la definición formal a continuación.

\newpage
```{=tex}
\begin{definicion}[Priori conjugada]
  Decimos que una clase $\mathcal{P}$ de distribuciones a priori es \textbf{conjugada} para la verosimilitud $f(\mathbf{y}|\theta)$ si
  $$
    \pi(\theta) \in \mathcal{P} \implies \pi(\theta|\mathbf{y}) \in \mathcal{P}
  $$
  esto es, la distribución a posteriori de $\theta$ sigue teniendo la misma distribución que la priori.
\end{definicion}
```

Los modelos conjugados son convenientes ya que obtenemos resultados analíticamente tractables, así como generalmente intuitivos. Ahora, nada nos debe restringir a ocupar modelos conjugados. En particular, vemos que, en un principio, tanto para la verosimilitud como para la priori podemos ocupar cualquier distribución de probabilidad, que deberán ser elegidas de acorde al problema.

Lo anterior provocó uno de los cuellos de botella más importantes de la estadística Bayesiana, que frenó su amplio uso en la práctica. Principalmente, el problema radicaba en los cálculos computacionales necesarios para realizar la inferencia a posteriori.

* Métodos computacionales
  * Rejection Sampling
  * Importance Sampling
  * MCMC: Gibbs sampler, Slice sampling
  * MCMC: Metropolis y Metropolis-Hastings
  * MCMC: Hamiltonian Monte Carlo and No-U-Turn Sampler
  * MCMC: Sequential Monte Carlo

* Lenguajes probabilísticos
  * Stan
  * Turing
  * PyMC3

## Estadística No-Paramétrica

Normalmente, en estadística asumimos

$$
y_1,..., y_n | G \overset{i.i.d.}{\sim} G
$$

Suponemos que la densidad de $G$, $g$, pertenece a 

$$
\mathcal{G} = \{g_\theta\colon \theta \in \Theta \subset \mathbb{R}^p\}
$$

* Ejemplo

* Figura

::: {#fig-np layout-ncol=2}
![](figures/NP%20-%20Example%201.jpg)

![](figures/NP%20-%20Example%202.jpg)

Necesidad de métodos flexibles
:::

* Nos gustaría ir un poco más allá: estimación de densidades (figura) y regresión

## Estadística Bayesiana No-Paramétrica

\newpage
# Procesos de Dirichlet

## Definición

Consideremos en primer lugar el problema de estimación de densidades ...

Considerando lo anterior, debemos entonces definir medidas de probabilidad sobre medidas de probabilidad. Una de estas opciones es la del Proceso de Dirichlet, que definimos a continuación.

```{=tex}
% Definición Proceso de Dirichlet
\begin{definicion}[(Ferguson, 1973)]
  Sea $\alpha>0$ y $G_0$ una medida de probabilidad definida sobre $S$. Un \textbf{Proceso de Dirichlet (DP)} de parámetros $(\alpha, G_0)$, denotado por $\text{DP}(\alpha, G_0)$, es una medida de probabilidad aleatoria $G$ definida en $S$ que asigna probabilidad $G(B)$ a todo conjunto medible $B$ tal que, para toda partición medible finita $\{B_1, ..., B_k\}$ de $S$, la distribución conjunta del vector $(G(B_1), ..., G(B_k))$ es Dirichlet con parámetros

$$
(\alpha G_0(B_1), ..., \alpha G_0(B_k))
$$


Los parámetros $G_0$ y $\alpha$ se denominan la \textbf{medida de centralización} y la \textbf{precisión}, respectivamente. También se suele denominar $\alpha G_0$ como la \textbf{medida base}.
\end{definicion}
```

Ferguson muestra que $G$ existe para todo $G_0$. Además, señala algunas de las propiedades estadísticas de los DP, tales como:

```{=tex}
\begin{propiedad}[(Propiedades Proceso de Dirichlet)]
  Sea $G \sim \text{DP}(\alpha, G_0$), $B, B_1$ y $B_2$ conjuntos medibles, con $B_1 \cap B_2 = \emptyset$. Luego,
  \begin{itemize}
    \item El soporte de $G$ coincide con el de $G_0$. Esto es,
    \begin{equation*}
      G_0(B) = 0 \implies P(G(B) = 0) = 1
    \end{equation*}
    y
    \begin{equation*}
      \quad G_0(B) > 0 \implies P(G(B) > 0) > 1 
    \end{equation*}
    \item $\text{E}(G(B)) = G_0(B)$
    \item $\text{Var}(G(B)) = \frac{G_0(B)(1 - G_0(B)}{1 + \alpha}$
    \item $\text{Cov}(G(B_1), G(B_2)) = \frac{-G_0(B_1)G_0(B_2)}{1 + \alpha}$
  \end{itemize}
\end{propiedad}
```

Las propiedades anteriores nos muestran la razón por la que $G_0$ se denomina la medida de centralización, así como el por qué $\alpha$ se denomina el parámetro de precisión. La covarianza muestra que la covarianza entre dos conjuntos cualesquiera es siempre negativa (acá hay extensiones, mencionar que están fuera del alcance de este trabajo)

```{=tex}
\begin{nota}
  Algunos se preguntarán, como yo lo hice la primera vez que aprendí sobre esto, el por qué se denomina un \textbf{proceso}. La razón es bastante sencilla, y es que el DP es un proceso estocástico que, en vez de estar indexado por índices comunes como el tiempo o coordenadas geográficas, está indexado por todos los conjuntos medibles, esto es, para cada conjunto medible $B$ tenemos la variable aleatoria $G(B)$.
\end{nota}
```

Una propiedad muy importante que muestra Ferguson, que será central en el transcurso de esta monografía, es que $G$ es casi-seguramente discreta. Este resultado nos dice que $G$ se puede escribir como una suma ponderada de masas puntuales, también denominados átomos, esto es,

$$
G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot)
$$

donde $\sum_{h=1}^\infty w_h = 1$ y $\delta_{x}(\cdot)$ denota la medida de Dirac en $x$. En la Figura 2 se presenta gráficamente un Proceso de Dirichlet. A la izquierda se ilustran los átomos con puntos morados, donde los largos indican la masa que aporta cada uno. A la derecha se muestra cómo se calcularía la probabilidad para un cierto conjunto medible $B$, que es simplemente tomar la suma de las masas de los átomos que se encuentran dentro de este conjunto.

::: {#fig-dp layout-ncol=2}
![](figures/DP%20-%20Definition.jpg)

![](figures/DP%20-%20Definition%202.jpg)

Naturaleza discreta del Proceso de Dirichlet
:::

Por último, Ferguson tambien demuestra que un DP es conjugada para una muestra i.i.d. de esta distribución, donde se considera un promedio ponderado entre la medida de centralización $G_0$ y la función de distribución empírica de los datos.

```{=tex}
% Posteriori Proceso de Dirichlet
\begin{proposicion}[(Ferguson, 1973)]
  Sea $y_1, ..., y_n | G \overset{i.i.d}{\sim} G$ y $G \sim \text{DP}(\alpha, G_0)$. Luego,
  \begin{equation*}
    G | y_1, ..., y_n \sim \text{DP}\left(\alpha + n, \frac{\alpha G_0 + n \hat{f}_n}{\alpha + n}\right)
  \end{equation*}
  donde $\hat{f}_n$ es la distribución empírica obtenida a partir de los datos, i.e.
  \begin{equation*}
    \hat{f}_n(\cdot) = \frac{1}{n}\sum_{i=1}^n \delta_{y_i}(\cdot)
  \end{equation*}
\end{proposicion}
```

Ahora, todo lo anterior aún no nos dice mucho acerca de como trabajar con esta distribución, ya que de momento solo sabemos que existen tales procesos, así como algunas propiedades. En la práctica, se ha trabajado principalmente de dos formas. La primera es marginalizando la medida de probabilidad aleatoria $G$, esto es, trabajar directamente con

$$
p(y_1, ..., y_n) = \int p(y_1, ..., y_n|G) d\pi(G)
$$

La otra forma es considerar la construcción de un DP mediante una representación basada en cortar una varilla de largo unitario de manera sucesiva e indefinida, denominada *Stick-Breaking*.

## Construcción mediante Urnas de Pólya

Una de las formas de poder trabajar con un Proceso de Dirichlet es, irónicamente, no trabajar con él. En probabilidades esto lo logramos marginalizando con respecto a la medida que no es de interés.

Considerando una muestra aleatoria $y_1, ..., y_n|G \sim G$, Blackwell y MacQueen  [@blackwell_ferguson_1973] formulan una representación de la densidad marginal $p(y_1, ..., y_n)$ mediante una representación por Urnas de Pólya. En particular, se tiene que

$$
p(y_1, ..., y_n) = p(y_1)\prod_{i=2}^n p(y_i|y_1, ..., y_{i-1})
$$

y lo que muestran es que

$$
p(y_i|y_1, ..., y_{i-1}) = \frac{1}{M + i - 1}\sum_{h=1}^{i-1}\delta_{y_h}(y_i) + \frac{M}{M + i - 1}G_0(y_i)
$$

Hay dos cosas bastante importantes:

* Dada la intercambiabilidad, las condicionales completas toman la misma forma
* La predictiva toma la misma forma para $i = n + 1$
* Lo anterior se puede simplificar considerando solo los valores iguales

```{=tex}
\begin{ejemplo}[Urnas de Pólya]
  Para ilustrar, consideramos el ejemplo de obtener datos de un Proceso de Dirichlet con medida de centralización Gamma(6, 4) y precisión $\alpha = 1, 10, 50, 100, 1000, 10000$.
\end{ejemplo}
```


```{julia}
#| label: fig-crp-gamma
#| echo: false
#| fig-cap: Simulación de datos provenientes de un Proceso de Dirichlet con medida de centralización Gamma(6, 4) (en azul)
#| fig-subcap:
#|   - $\alpha$ = 1
#|   - $\alpha$ = 10
#|   - $\alpha$ = 50
#|   - $\alpha$ = 100
#|   - $\alpha$ = 1000
#|   - $\alpha$ = 10000
#| layout-ncol: 3
#| layout-nrow: 2

include("../src/00_extras.jl")
include("code/c2_f.jl")

Random.seed!(219);
G0 = Distributions.Gamma(6, 1/4);
display(tic_rdp_marginal_example(500, 1, G0))
display(tic_rdp_marginal_example(500, 10, G0))
display(tic_rdp_marginal_example(500, 50, G0))
display(tic_rdp_marginal_example(500, 100, G0))
display(tic_rdp_marginal_example(500, 1000, G0))
display(tic_rdp_marginal_example(500, 10000, G0))
```

## Construcción Stick-Breaking

La forma de trabajar directamente con un Proceso de Dirichlet vino dada por una construcción stick-breaking indefinida.

```{=tex}
\begin{theo}[(Sethuraman, 1994)]
  Sea $w_h = \upsilon \prod_{l<h} (1 - \upsilon_l)$ con $\upsilon_h \overset{i.i.d.}{\sim} \text{Beta}(1, \alpha)$ y $m_h \overset{i.i.d.}{\sim} G_0$, donde $(\upsilon_h)$ y $(m_h)$ son independientes entre sí. Luego,

  \begin{equation*}
    G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot)
  \end{equation*}
  
  define un Proceso de Dirichlet de parámetros $\alpha$ y $G_0$.
\end{theo}
```

En la Figura 4 se pequeña una pequeña ilustración del proceso stick-breaking para obtener un Proceso de Dirichlet.

::: {#fig-stickbreaking}
![](figures/DP%20-%20Stick%20Breaking.jpg)

Ilustación del proceso de Stick-Breaking
:::

Ahora, lo anterior sigue teniendo un pequeño problema, y es que claramente no podemos repetir el proceso una cantidad infinita de veces para obtener las secuencias infinitas de pesos y localizaciones. Para arreglar esto, se propone simplemente truncar la representación hasta un valor $H$ fijo, considerando $\upsilon_H = 1$, u obtener los pesos $w_h$ hasta cubrir un cierto número fijo, cercano a 1, de probabilidad.

```{=tex}
\begin{ejemplo}[Stick-Breaking]
\end{ejemplo}
```

```{julia}
#| label: fig-dp-normal
#| echo: false
#| fig-cap: Simulaciones Proceso de Dirichlet con medida de centralización Normal (en rojo)
#| fig-subcap:
#|   - $\alpha$ = 1
#|   - $\alpha$ = 10
#|   - $\alpha$ = 50
#|   - $\alpha$ = 100
#|   - $\alpha$ = 500
#|   - $\alpha$ = 1000
#| layout-ncol: 3
#| layout-nrow: 2

Random.seed!(219);
G0 = Distributions.Normal(0, 1)
display(tic_rdp_example(15, 1, G0, -10, 10, (-3, 3)))
display(tic_rdp_example(15, 10, G0, -10, 10, (-3, 3)))
display(tic_rdp_example(15, 50, G0, -10, 10, (-3, 3)))
display(tic_rdp_example(15, 100, G0, -10, 10, (-3, 3)))
display(tic_rdp_example(15, 500, G0, -10, 10, (-3, 3)))
display(tic_rdp_example(15, 1000, G0, -10, 10, (-3, 3)))
```

```{=tex}
\begin{nota}
  En este punto del trabajo fue donde decidí cambiarme de R a Julia, ya que las simulaciones anteriores tomaban demasiado tiempo. Utilizando Julia obtuve una mejoría en rapidez de casi 100 veces.
\end{nota}
```

## Algunos resultados asintóticos

Resultados de Antoniak, Korwar & Hollander.

```{julia}
#| echo: false

Random.seed!(219)
n_values = 1000:2000:21000

akh_empirical(n_values, 100)
```


\newpage
# Dirichlet Process Mixture Models

## Definición

Podemos notar de la sección anterior que obtener distribuciones discretas con probabilidad 1 puede no ser adecuado para diferentes problemas. Considerando lo anterior, nos interesa extender los procesos de Dirichlet para formular modelos adecuados a estos casos. Utilizaremos la notación de [@neal_markov_2000]

Una opción es usar estas medidas de probabilidad aleatoria como la *mixing distribution* en un modelo de mezcla, esto es,

$$
f_G(y) = \int f_\theta(y)dG(\theta)
$$

Equivalentemente:

$$
\begin{aligned}
y_i|\theta_i &\overset{ind.}{\sim} F(\theta_i) \\
\theta_i|G &\overset{i.i.d.}{\sim} G \\
G &\sim \text{DP}(\alpha, G_0)
\end{aligned}
$$

**Resultado de Antoniak**

De manera más general podemos incluir inferencia sobre $\alpha$ e hiperparámetros de la medida de centro $G_0$. Procedemos entonces con el siguiente modelo:

$$
\begin{aligned}
  y_i|\theta_i &\overset{ind.}{\sim} F(\theta_i) \\
  \theta_i | G &\overset{i.i.d.}{\sim} G \\
  G &\sim \text{DP}(\alpha, G_\eta) \\
  (\alpha, \eta) &\sim \pi
\end{aligned}
$${#eq-dpm}


## Simulación a posteriori

* Muestreo de Gibbs [@geman_stochastic_1984]

* Posteriori condicional completa de $\theta$.

$$
\theta_i | \theta_{-i}, y_i \sim \sum_{j\neq i}q_{i,j}\delta(\theta_j) + r_i H_i
$$

donde

$$
\begin{aligned}
  q_{i,j} &= bF(y_i, \theta_j) \\
  r_i &= b\alpha \int F(y_i, \theta)dG_0(\theta)
\end{aligned}
$$

donde $b$ es tal que $\sum_{j\neq i}q_{i,j} + r_i = 1$ y $H_i$ es la posteriori que se obtiene al considerar un modelo con $G_0$ como priori y una única observación de la verosimilitud $F(y_i, \theta_i)$.

* Resultado $\alpha$: depende solo del número de valores únicos en $\theta$.
* Resultado $\eta$: proporcional a su priori multiplicada por $\prod_{c} G_0(\theta_i)$.

### Caso conjugado

De la ecuación anterior hay dos problemas:

* Simular de $H_i$
* Calcular la integral $\int F(y_i, \theta)dG_0(\theta)$

Así, en primera instancia se consideraron modelos conjugados.

#### Escobar & West (1995)

```{=tex}
\begin{ejemplo}[(Datos de Galaxias)]
Modelo propuesto por Escobar y West en el paper del 1995

To finalize the article, the authors present a final extension of the previous algorithm that now includes learning about the precision parameter $\alpha$.

This final model is represented as,

$$
\begin{aligned}
  Y_i | \pi_i &\overset{ind.}{\sim} \text{N}(\mu_i, V_i), \quad i = 1, ..., n \\
  \pi_1, ..., \pi_n &\overset{i.i.d.}{\sim} G  \\
  G &\sim DP(\alpha, G_0) \\
  G_0 &= N-\Gamma^{-1}(m, 1/\tau, s/2, S/2) \\
  \tau &\sim \Gamma^{-1}(w/2, W/2) \\
  m &\sim N(0, A), \quad A \to \infty \\
  \alpha & \sim \Gamma(a, b) 
\end{aligned}
$$

It follows that,

* The full conditional of $\alpha$ is given by,

$$
\alpha | \pi, m, \tau, D_n \equiv \alpha | \eta, k \sim \pi_\eta \Gamma(a + k, b - \log \eta) + (1 - \pi_\eta)\Gamma(a + k - 1, b - \log \eta)
$$

where $\pi_\eta/(1 - \pi_\eta) = (a + k - 1)/[n (b - \log \eta)]$. Here we introduced an auxiliary variable $\eta$ that satisfies

$$
\eta | \alpha, k \sim \text{Beta}(\alpha + 1, n)
$$

* The full conditional of $m$ is given by,

$$
m | \tau, \pi, \alpha, D_n \equiv m | \pi, \tau \sim N\Big[ x\bar{V}\sum(V_j^*)^{-1}\mu_j^*\,;\, x\tau\bar{V}\Big]
$$

  where $x = A/(A + \tau \bar{V})$ and $\bar{V}^{-1} = \sum(V_j^*)^{-1}$.

* The full conditional of $\tau$ is given by,

$$
\tau | \pi, m, \alpha, D_n \equiv \tau | \pi, m \sim \Gamma^{-1}((w + k)/2, (W + K)/2)
$$

  where $K = \sum_{j=1}^k (\mu_j^* - m)^2/V_j^*$.

* The full conditionals of $\pi$ are given by,

$$
\pi_i|\pi^{(i)}, m, \tau, \alpha, D_n \sim q_0G_i(\pi_i) + \sum_{j\neq i}
q_j \delta_{\pi_j}(\pi_i)
$$

  where $G_i(\pi_i) \equiv N-\Gamma^{-1}(x_i, 1/X; (1 + s)/2, S_i/2)$ and 

$$
    \begin{aligned}
    q_0 &\propto \alpha c(s)[1 + (y_i - m)^2/(sM)]^{-(1 + s)/2}/M^{1/2} & \propto \alpha \cdot t_s(m, \sqrt{M})\\
    q_j &\propto \exp[-(y_i - \mu_j)^2/(2V_j)](2V_j)^{-1/2} &\propto N(\mu_j, V_j) \\
    \sum_{j=0, j\neq i}^n q_j &= 1 
    \end{aligned}
$$

  with

  -   $x_i = (m + \tau y_i)/(1 + \tau)$
  
  -   $X = \tau/(1 + \tau)$
  
  -   $S_i = S + (y_i - m)^2/(1 + \tau)$
  
  -   $M = (1 + \tau)S/s$
  
  -   $c(s) = \Gamma((1 + s)/2)\Gamma(s/2)^{-1}s^{-1/2}$

Thus, the algorithm proceeds as follows:

1. Sample initial values in the following way:
    * Sample $\alpha \sim \Gamma(a, b)$
    * Sample $\tau \sim \Gamma^{-1}(w/2, W/2)$
    * Sample $m \sim N(0, 1)$
    * Sample $\pi$ given $m, \tau$
2. For $t = 1, ..., N$:
    * Sample $\eta_{(t)}|\alpha_{(t-1)}, k_{(t-1)}$
    * Sample $\alpha_{(t)} | \eta_{(t)}, k_{(t-1)}$
    * Sample $m_{(t)} | \pi_{(t-1)}, \tau_{(t-1)}$
    * Sample $\tau_{(t)} | \pi_{(t-1)}, m_{(t)}$
    * Sample $\pi_{i, (t)} | \pi^{(i)}_{(t-1)}, m_{(t)}, \tau_{(t)}, \alpha_{(t)}, D_n$ from its full conditional. Be aware that $\pi^{(i)}_{(t-1)}$ may contain already updated values of the form $\pi_{1, (t)}, ..., \pi_{i-1, (t)}, \pi_{i+1, (t-1)}, ..., \pi_{n, (t-1)}$.
\end{ejemplo}
```

```{julia}
#| label: fig-galaxias1
#| echo: false
#| fig-cap: Velocidades de galaxias

include("code/c3_f.jl")

velocities = [9172, 9558, 10406, 18419, 18927, 19330, 19440, 19541,
    19846, 19914, 19989, 20179, 20221, 20795, 20875, 21492,
    21921, 22209, 22314, 22746, 22914, 23263, 23542, 23711,
    24289, 24990, 26995, 34279, 9350, 9775, 16084, 18552,
    19052, 19343, 19473, 19547, 19856, 19918, 20166, 20196,
    20415, 20821, 20986, 21701, 21960, 22242, 22374, 22747,
    23206, 23484, 23666, 24129, 24366, 25633, 32065, 9483,
    10227, 16170, 18600, 19070, 19349, 19529, 19663, 19863,
    19973, 20175, 20215, 20629, 20846, 21137, 21814, 22185,
    22249, 22495, 22888, 23241, 23538, 23706, 24285, 24717,
    26960, 32789] ./ 1000;

histogram(velocities, bins=1:40, label="")
```

```{julia}
#| label: fig-pos-ew
#| echo: false
#| fig-cap: Densidades estimadas a posteriori
#| fig-subcap:
#|   - Predictiva a posteriori
#|   - $\alpha$|D
#| layout-ncol: 2
#| layout-nrow: 1

a, b, A, w, W, s, S = 2, 4, 1000, 1, 100, 4, 2;
prior_par = (a, b, A, w, W, s, S);

Random.seed!(219);
N = 4000;
warmup = 2000;
g_eta, g_alpha, g_mt, g_pi = tic_dpm_ew(velocities, prior_par, N, warmup);

n = length(velocities)
function cond_dens(y)
    s1 = [
        g_alpha[i] * pdf(TDist(s), (y - g_mt[i, 1]) / sqrt(1 + g_mt[i, 2] * S / s)) /
        sqrt(1 + g_mt[i, 2] * S / s) for i in 1:(N-warmup)
    ]

    s2 = [
        sum(map(x -> pdf(Normal(x[1], sqrt(x[2])), y), eachrow(sample)))
        for sample in eachslice(g_pi, dims=1)
    ]

    dens = (s1 .+ s2) ./ (g_alpha .+ n)

    return mean(dens)
end

y_grid = range(8, 40, length=500);
dens_est = [cond_dens(y) for y in y_grid];

histogram(velocities, bins=1:40, label="", normalize=true);
display(plot!(y_grid, dens_est, label="Densidad estimada", linewidth=2.5))


function cond_dens(alpha)
    function a_dist(i, alpha)
        eta = g_eta[i]

        unique_pi = unique(g_pi[i, :, :], dims=1)
        k = size(unique_pi)[1]
        odds_w = (a + k - 1) / (n * (b - log(eta)))
        weight = odds_w / (1 + odds_w)

        weight * pdf(Gamma(a + k, 1 / (b - log(eta))), alpha) +
        (1 - weight) * pdf(Gamma(a + k - 1, 1 / (b - log(eta))), alpha)
    end

    dens = [a_dist(i, alpha) for i in 1:(N-warmup)]
    return mean(dens)
end

alpha_grid = range(0, 3, length=500);
dens_est = [cond_dens(alpha) for alpha in alpha_grid]
plot(alpha_grid, dens_est, label="Estimated density", linewidth=3);
display(plot!(alpha_grid, pdf(Gamma(a, 1 / b), alpha_grid), label="Prior density"))
```

#### Problema de "sticky-clusters"

Problema: sticky clusters

```{julia}
#| label: fig-sticky-clusters
#| echo: false
#| fig-cap: Simulación a post de un parámetro. Vemos que se queda pegado

aux_theta = g_pi[1:end, 1, 1]
plot(1:500, aux_theta[1:500])
```

Bush & MacEachern, separación de valores.

**Figura de cómo recuperar valores utilizando las indicadoras de clusters**





### Caso no-conjugado

* No-gaps de MacEachern & Muller
* Algoritmo 8 de Neal


### Otras opciones

Inferencia variacional, DP $\epsilon$-finito.


\newpage
# Particiones Aleatorias y Clustering

\newpage
# Aplicación: Modelo CAPM

* El modelo de valorización de activos financieros (CAPM) fue desarrollado en los años 60 de forma independiente por Jack Treynor, William Sharpe, John Linter y Jan Mossin.

* El modelo propone 

$$
\text{E}(R) = r_f + \beta(\text{E}(R_m) - r_f)
$$

donde R es el retorno del activo, $r_f$ es la tasa de retorno libre de riesgo, $R_m$ es el retorno del mercado y $\beta$ es el riesgo sistemático del activo bajo estudio.

* Normalmente se considera el modelo de regresión

$$
Y_j \equiv r_j - r_{fj} = \alpha + \beta(r_{mj} - r_{fj}) + \varepsilon_j, \quad j = 1,...,n
$$

\newpage
# Referencias
<!-- ---
nocite: |
  @*
--- -->
