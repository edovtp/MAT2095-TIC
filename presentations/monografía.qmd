---
title: "MAT2035 - Taller de Iniciación Científica"
author: "Eduardo Vásquez"
number-sections: true
highlight-style: pygments
format: 
  html:
    toc: true
    code-fold: true
  pdf:
    documentclass: article
    toc: true
    include-in-header: header.tex
    toc-title: "Tabla de Contenidos"
execute: 
  cache: true
bibliography: references.bib
---

\newpage

\section*{Prefacio}

El objetivo de este taller es implementar el algoritmo SIGN [@ni_scalable_2020], que permite aplicar modelos de Mezcla Proceso de Dirichlet (DPM) para bases de datos grandes.

Todo este material fue confeccionado durante mi Taller de Iniciación Científica junto al profesor Fernando Quintana, durante el segundo semestre del 2022. Todo el código y material se encuentra disponible en el siguiente [repositorio](https://github.com/edovtp/MAT2095-TIC) de Github.

\newpage
# Introducción

El material presentado a continuación se enmarca en el área de la Estadística Bayesiana No-Paramétrica, por lo que, en primer lugar, es necesario presentar una breve introducción de ambos conceptos.

## Paradigma Bayesiano

Es común que al presentarnos como estadísticos se nos pregunte acerca de qué es lo que hacemos en nuestro trabajo. Ante esta pregunta, comúnmente respondemos que nuestro objetivo principal es el de cuantificar la incertidumbre. Explicamos, además, que para lo anterior nos apoyamos sobre la teoría de probabilidades, tomándola como herramienta principal para modelar aquellas incertezas de interés.

Pero, como estadísticos podemos ir un paso atrás, y preguntarnos acerca de a qué nos referimos exactamente con incertidumbre. Esta pregunta es la que nos lleva a las bases mismas de la estadística, así como a entender cómo surgieron dos visiones diferentes entre sí: la *Estadística Frecuentista* (también denominada clásica) y la *Estadística Bayesiana*.

En particular, Tony O'Hagan distingue dos tipos de incertidumbre [@ohagan_dicing_2004]. Una de ellas la podemos denominar *incerteza ontólogica* (o aleatoria), mientras que la otra toma el nombre de *incerteza epistemológica*. Es importante recordar que la ontología es el estudio del *ser*, mientras que la epistemología es el estudio del *saber*.

La incerteza ontológica trata acerca de una incerteza que está sujeta a una variabilidad aleatoria innata, que no podemos predecir bajo ninguna cantidad de información. Dentro de los ejemplos de incerteza ontológica se encuentran varios de los ejemplos introductorios a la estadística, como el lanzamiento de un dado o ganar la lotería.

Por otro lado, la incerteza epistemológica, tal como lo dice el nombre, es una incerteza acerca de lo que sabemos, como puede ser el...

Probabilidades subjetivas...

Como en estadística Bayesiana también consideramos medir mediante probabilidades los parámetros de nuestros modelos, debemos definir entonces un modelo conjunto de cantidades observables y no observables. Esto es,

$$
p(\mathbf{y}, \theta) = p(\mathbf{y}|\theta)\pi(\theta)
$$

Luego, a la luz de nueva información, se actualiza nuestra creencia a priori, mediante el teorema de Bayes, para obtener entonces la distribución a posteriori.

\begin{equation}\label{eq:1}
\begin{aligned}
\pi(\theta|\mathbf{y}) &= \frac{f(\mathbf{y}|\theta)\pi(\theta)}{\int_{\Theta}f(\mathbf{y}|\theta)\pi(\theta)d\theta} \\
&\propto f(\mathbf{y}|\theta)\pi(\theta)
\end{aligned}
\end{equation}

\begin{ejemplo}[Modelo Normal-Normal]
\end{ejemplo}

En el ejemplo anterior obtuvimos un resultado bastante conveniente. Al considerar una función de verosimilitud Normal, así como una priori Normal para $\theta$, obtuvimos que a posteriori la distribución sigue siendo una distribución Normal, con la actualización de los parámetros dada en (poner ecuación). Este tipo de modelos se denominan **conjugados**...

Ahora, nada nos debe restringir a ocupar modelos conjugados solo para poder trabajarlos de forma más fácil con resultados analíticos. En particular, vemos que, en un principio, tanto para la verosimilitud como para la priori podemos ocupar cualquier distribución de probabilidad. 

Uno de los cuellos de botella más importantes de la estadística bayesiana, que provocó el poco uso de esta...

* Métodos computacionales
  * Rejection Sampling
  * Importance Sampling
  * MCMC: Gibbs sampler, Slice sampling
  * MCMC: Metropolis y Metropolis-Hastings
  * MCMC: Hamiltonian Monte Carlo and No-U-Turn Sampler
  * MCMC: Sequential Monte Carlo

* Lenguajes probabilísticos
  * Stan
  * Turing
  * PyMC3

## Estadística No-Paramétrica

Normalmente, en estadística asumimos

$$
y_1,..., y_n | G \overset{i.i.d.}{\sim} G
$$

Suponemos que la densidad de $G$, $g$, pertenece a 

$$
\mathcal{G} = \{g_\theta\colon \theta \in \Theta \subset \mathbb{R}^p\}
$$

* Ejemplo

* Figura

::: {#fig-np layout-ncol=2}
![](figuras/NP%20-%20Example%201.jpg)

![](figuras/NP%20-%20Example%202.jpg)

Necesidad de métodos flexibles
:::


* Nos gustaría ir un poco más allá: estimación de densidades (figura) y regresión

\newpage
# Procesos de Dirichlet

Considerando lo anterior, debemos entonces definir medidas de probabilidad sobre medidas de probabilidad. Una de estas opciones es la del proceso de Dirichlet [@ferguson_bayesian_1973], que definimos a continuación.

\begin{definicion}[(Ferguson, 1973)]
Sea $\alpha>0$ y $G_0$ una medida de probabilidad definida sobre $S$. Un \textbf{Proceso de Dirichlet (DP)} de parámetros $(\alpha, G_0)$, denotado por $\text{DP}(\alpha, G_0)$, es una medida de probabilidad aleatoria $G$ definida en $S$ que asigna probabilidad $G(B)$ a todo conjunto medible $B$ tal que, para toda partición medible finita $\{B_1, ..., B_k\}$ de $S$, la distribución conjunta del vector $(G(B_1), ..., G(B_k))$ es Dirichlet con parámetros

$$
(\alpha G_0(B_1), ..., \alpha G_0(B_k))
$$
\end{definicion}

Los parámetros $G_0$ y $\alpha$ se denominan la medida de 

Ferguson muestra que $G$ existe para todo $G_0$. Además, señala algunas de las propiedades estadísticas de los DP, tales como:

* La esperanza de $G(B)$ para cualquier conjunto medible $B$ es $G_0(B)$, lo cual nos indica la razón por la cual $G_0$ se denomina la medida de centrado (?). Esto es,
$$
\text{E}(G(B)) = G_0(B)
$$

* La varianza de $G(B)$ es $G_0(B)(1 - G_0(B))/(1 + \alpha)$, lo cual nos indica la razón por la cual $\alpha$ se denomina el parámetro de precisión.

* Dado dos conjuntos medibles disjuntos $B_1$ y $B_2$, la covarianza de $G(B_1)$ y $G(B_2)$ corresponde a $-G_0(B_1)G_0(B_2)/(1 + \alpha)$. Esto nos indica que la covarianza es **siempre** negativa, independiente de la cercanía o lejanía que presenten estos dos conjuntos.

Por último, otra propiedad importante que muestra Ferguson es que $G$ es casi-seguramente discreta. Este resultado nos dice que $G$ se puede escribir como una suma ponderada de masas puntuales, también denominados átomos, esto es,

$$
G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot)
$$

donde $\sum_{h=1}^\infty w_h = 1$ y $\delta_{x}(\cdot)$ denota la medida de Dirac en $x$. Gráficamente:

::: {#fig-dp layout-ncol=2}
![](figuras/DP%20-%20Definition.jpg)

![](figuras/DP%20-%20Definition%202.jpg)

Naturaleza discreta del Proceso de Dirichlet (DP)
:::

Ahora, todo lo anterior aún no nos dice mucho acerca de como trabajar con esta distribución, ya que de momento solo sabemos que existen tales procesos, así como algunas propiedades. En la práctica, se ha trabajado principalmente de dos formas: marginalizando 

* Marginalizar: urnas de Pólya
* Stick-breaking

## Construcción con Urnas de Pólya

[@blackwell_ferguson_1973]

```{julia}
#| label: fig-crp-normal
#| echo: false
#| fig-cap: Simulación de datos provenientes de un Proceso de Dirichlet
#| fig-subcap:
#|   - M = 1
#|   - M = 10
#|   - M = 50
#|   - M = 100
#|   - M = 1000
#|   - M = 10000
#| layout-ncol: 3
#| layout-nrow: 2

include("../src/00_extras.jl")
include("../src/01_DP.jl")
include("../src/02_DPM.jl")
include("../src/03_DPM_EW.jl")
include("../src/04_DPM_Neal.jl")

# Example CRP - Normal centering measure
Random.seed!(219);
G0 = Distributions.Normal(0, 1);

display(tic_rdp_marginal_example(500, 1, G0))
display(tic_rdp_marginal_example(500, 10, G0))
display(tic_rdp_marginal_example(500, 50, G0))
display(tic_rdp_marginal_example(500, 100, G0))
display(tic_rdp_marginal_example(500, 1000, G0))
display(tic_rdp_marginal_example(500, 10000, G0))
```


## Stick-Breaking

**Nota**: mejoría de casi 100 segundos con respecto a R

Distribución Normal

```{julia}
#| label: fig-dp-normal
#| echo: false
#| fig-cap: Simulaciones Proceso de Dirichlet con medida de centralización Normal (en rojo)
#| fig-subcap:
#|   - M = 1
#|   - M = 10
#|   - M = 50
#|   - M = 100
#|   - M = 500
#|   - M = 1000
#| layout-ncol: 3
#| layout-nrow: 2
# include("../src/01_DP.jl")

# Random.seed!(219);
# G0 = Distributions.Normal(0, 1)
# display(tic_rdp_example(15, 1, G0, -10, 10, (-3, 3)))
# display(tic_rdp_example(15, 10, G0, -10, 10, (-3, 3)))
# display(tic_rdp_example(15, 50, G0, -10, 10, (-3, 3)))
# display(tic_rdp_example(15, 100, G0, -10, 10, (-3, 3)))
# display(tic_rdp_example(15, 500, G0, -10, 10, (-3, 3)))
# display(tic_rdp_example(15, 1000, G0, -10, 10, (-3, 3)))
```

Distribución Gamma

```{julia}
#| label: fig-dp-gamma
#| echo: false
#| fig-cap: Simulaciones Proceso de Dirichlet con medida de centralización Gamma (en rojo)
#| fig-subcap:
#|   - M = 1
#|   - M = 10
#|   - M = 50
#|   - M = 100
#|   - M = 500
#|   - M = 1000
#| layout-ncol: 3
#| layout-nrow: 2
# Random.seed!(219);
# G0 = Distributions.Gamma(6, 1 / 4)
# display(tic_rdp_example(15, 1, G0, -10, 10, (0, 4)))
# display(tic_rdp_example(15, 10, G0, -10, 10, (0, 4)))
# display(tic_rdp_example(15, 50, G0, -10, 10, (0, 4)))
# display(tic_rdp_example(15, 100, G0, -10, 10, (0, 4)))
# display(tic_rdp_example(15, 500, G0, -10, 10, (0, 4)))
# display(tic_rdp_example(15, 1000, G0, -10, 10, (0, 4)))
```

## Algunos resultados asintóticos

Results from Antoniak, Korwar & Hollander.

```{julia}
#| echo: false
# function akh_empirical(n_values, M)
#     # Expected value for k
#     ek_values = Vector{Float64}(undef, length(n_values))

#     function n_unique(n, M)
#         sim_data = tic_rdp_marginal(n, M, Normal(0, 1))
#         k = length(unique(sim_data))
#     end

#     for (i, n) in enumerate(n_values)
#         k_sim = [n_unique(n, M) for _ in 1:20]
#         ek_values[i] = mean(k_sim)
#     end

#     result = plot(x -> M * log(x), 1, maximum(n_values) + 100,
#         label="Korwar & Hollander", size=(800, 600),
#         legend=:bottomright)
#     plot!(result, x -> M * log(1 + x / M), label="Antoniak")
#     scatter!(result, n_values, ek_values, label="Simulation")
# end


# Random.seed!(219)
# n_values = 1000:2000:21000

# akh_empirical(n_values, 1000)
```


\newpage
# Dirichlet Process Mixture Models

## Definición

## Simulación a posteriori

Gibbs:

* Caso conjugado:
  * Escobar (1988, 1994)
  * Escobar & West (1995)
  * Bush & MacEachern (1996)
* Caso no-conjugado:
  * No-gaps, MacEachern & Müller (1998)
  * Algoritmo 8, Neal (2000)

Otros: slice sampler, DP $\epsilon$-finito, inferencia variacional, etc.


\newpage
# Particiones Aleatorias y Clustering

\newpage
# Aplicación

aaaa

\newpage
# Referencias
---
nocite: |
  @*
---
