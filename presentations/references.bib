
@book{gelman_bayesian_2014,
	location = {Boca Raton},
	edition = {Third edition},
	title = {Bayesian data analysis},
	isbn = {978-1-4398-4095-5},
	series = {Chapman \& Hall/{CRC} texts in statistical science},
	abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
	pagetotal = {661},
	publisher = {{CRC} Press},
	author = {Gelman, Andrew},
	date = {2014},
	keywords = {Bayesian statistical decision theory, {MATHEMATICS} / Probability \& Statistics / General},
}

@book{muller_bayesian_2015,
	location = {Cham},
	title = {Bayesian Nonparametric Data Analysis},
	isbn = {978-3-319-18967-3 978-3-319-18968-0},
	url = {http://link.springer.com/10.1007/978-3-319-18968-0},
	series = {Springer Series in Statistics},
	publisher = {Springer International Publishing},
	author = {Müller, Peter and Quintana, Fernando Andres and Jara, Alejandro and Hanson, Tim},
	urldate = {2022-07-23},
	date = {2015},
	doi = {10.1007/978-3-319-18968-0},
	file = {(Springer Series in Statistics) Peter Müller, Fernando Andres Quintana, Alejandro Jara, Tim Hanson (auth.) - Bayesian Nonparametric Data Analysis-Springer International Publishing (2015).pdf:/mnt/Datos/Estadística UC/MAT2095 - Taller de Iniciación Científica/Libros/(Springer Series in Statistics) Peter Müller, Fernando Andres Quintana, Alejandro Jara, Tim Hanson (auth.) - Bayesian Nonparametric Data Analysis-Springer International Publishing (2015).pdf:application/pdf;Snapshot:/home/edovt/Zotero/storage/SDAAUN5V/978-3-319-18968-0.html:text/html},
}

@article{ni_scalable_2020,
	title = {Scalable Bayesian Nonparametric Clustering and Classification},
	volume = {29},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2019.1624366},
	doi = {10.1080/10618600.2019.1624366},
	abstract = {We develop a scalable multistep Monte Carlo algorithm for inference under a large class of nonparametric Bayesian models for clustering and classification. Each step is “embarrassingly parallel” and can be implemented using the same Markov chain Monte Carlo sampler. The simplicity and generality of our approach make inference for a wide range of Bayesian nonparametric mixture models applicable to large datasets. Specifically, we apply the approach to inference under a product partition model with regression on covariates. We show results for inference with two motivating datasets: a large set of electronic health records and a bank telemarketing dataset. We find interesting clusters and competitive classification performance relative to other widely used competing classifiers. Supplementary materials for this article are available online.},
	pages = {53--65},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Ni, Yang and Müller, Peter and Diesendruck, Maurice and Williamson, Sinead and Zhu, Yitan and Ji, Yuan},
	urldate = {2022-07-19},
	date = {2020-01-02},
	pmid = {32982129},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2019.1624366},
	keywords = {Electronic health records, Nonconjugate models, Parallel computing, Product partition models},
	file = {Accepted Version:/home/edovt/Zotero/storage/2PS5XEB7/Ni et al. - 2020 - Scalable Bayesian Nonparametric Clustering and Cla.pdf:application/pdf;Snapshot:/home/edovt/Zotero/storage/W3G2IEHL/10618600.2019.html:text/html},
}

@article{neal_markov_2000,
	title = {Markov Chain Sampling Methods for Dirichlet Process Mixture Models},
	volume = {9},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/1390653},
	doi = {10.2307/1390653},
	abstract = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
	pages = {249--265},
	number = {2},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Neal, Radford M.},
	urldate = {2022-07-19},
	date = {2000},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	file = {Neal, M. - Markov Chain Sampling Methods for Dirichlet Process Mixture Models.pdf:/mnt/Datos/Estadística UC/MAT2095 - Taller de Iniciación Científica/Artículos/Neal, M. - Markov Chain Sampling Methods for Dirichlet Process Mixture Models.pdf:application/pdf},
}

@article{escobar_bayesian_1995,
	title = {Bayesian Density Estimation and Inference Using Mixtures},
	volume = {90},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2291069},
	doi = {10.2307/2291069},
	abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models.},
	pages = {577--588},
	number = {430},
	journaltitle = {Journal of the American Statistical Association},
	author = {Escobar, Michael D. and West, Mike},
	urldate = {2022-07-19},
	date = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {Escobar, M. & West, M. - Bayesian Density Estimation and Inference Using Mixtures.pdf:/mnt/Datos/Estadística UC/MAT2095 - Taller de Iniciación Científica/Artículos/Escobar, M. & West, M. - Bayesian Density Estimation and Inference Using Mixtures.pdf:application/pdf},
}

@article{ohagan_dicing_2004,
	title = {Dicing with the unknown},
	volume = {1},
	issn = {1740-9713},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2004.00050.x},
	doi = {10.1111/j.1740-9713.2004.00050.x},
	abstract = {There are many things that I am uncertain about, says Tony O'Hagan. Some are merely unknown to me, while others are unknowable. This article is about different kinds of uncertainty, and how the distinction between them impinges on the foundations of Probability and Statistics.},
	pages = {132--133},
	number = {3},
	journaltitle = {Significance},
	author = {O'Hagan, Tony},
	urldate = {2022-11-09},
	date = {2004},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2004.00050.x},
	file = {Full Text PDF:/home/edovt/Zotero/storage/F4ZHS9MV/O'Hagan - 2004 - Dicing with the unknown.pdf:application/pdf;Snapshot:/home/edovt/Zotero/storage/ZLCZZIG7/j.1740-9713.2004.00050.html:text/html},
}

@article{blackwell_ferguson_1973,
	title = {Ferguson Distributions Via Polya Urn Schemes},
	volume = {1},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/Ferguson-Distributions-Via-Polya-Urn-Schemes/10.1214/aos/1176342372.full},
	doi = {10.1214/aos/1176342372},
	abstract = {The Polya urn scheme is extended by allowing a continuum of colors. For the extended scheme, the distribution of colors after \$n\$ draws is shown to converge as \$n {\textbackslash}rightarrow {\textbackslash}infty\$ to a limiting discrete distribution \${\textbackslash}mu{\textasciicircum}{\textbackslash}ast\$. The distribution of \${\textbackslash}mu{\textasciicircum}{\textbackslash}ast\$ is shown to be one introduced by Ferguson and, given \${\textbackslash}mu{\textasciicircum}{\textbackslash}ast\$, the colors drawn from the urn are shown to be independent with distribution \${\textbackslash}mu{\textasciicircum}{\textbackslash}ast\$.},
	pages = {353--355},
	number = {2},
	journaltitle = {The Annals of Statistics},
	author = {Blackwell, David and {MacQueen}, James B.},
	urldate = {2022-11-10},
	date = {1973-03},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Full Text PDF:/home/edovt/Zotero/storage/LKEIBB5X/Blackwell and MacQueen - 1973 - Ferguson Distributions Via Polya Urn Schemes.pdf:application/pdf;Snapshot:/home/edovt/Zotero/storage/TB7WKW6C/1176342372.html:text/html},
}

@article{ferguson_bayesian_1973,
	title = {A Bayesian Analysis of Some Nonparametric Problems},
	volume = {1},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/A-Bayesian-Analysis-of-Some-Nonparametric-Problems/10.1214/aos/1176342360.full},
	doi = {10.1214/aos/1176342360},
	abstract = {The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. ({II}) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which ({II}) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let \${\textbackslash}mathscr\{X\}\$ be a space and \${\textbackslash}mathscr\{A\}\$ a \${\textbackslash}sigma\$-field of subsets, and let \${\textbackslash}alpha\$ be a finite non-null measure on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$. Then a stochastic process \$P\$ indexed by elements \$A\$ of \${\textbackslash}mathscr\{A\}\$, is said to be a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with parameter \${\textbackslash}alpha\$ if for any measurable partition \$(A\_1, {\textbackslash}cdots, A\_k)\$ of \${\textbackslash}mathscr\{X\}\$, the random vector \$(P(A\_1), {\textbackslash}cdots, P(A\_k))\$ has a Dirichlet distribution with parameter \$({\textbackslash}alpha(A\_1), {\textbackslash}cdots, {\textbackslash}alpha(A\_k)). P\$ may be considered a random probability measure on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$, The main theorem states that if \$P\$ is a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with parameter \${\textbackslash}alpha\$, and if \$X\_1, {\textbackslash}cdots, X\_n\$ is a sample from \$P\$, then the posterior distribution of \$P\$ given \$X\_1, {\textbackslash}cdots, X\_n\$ is also a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with a parameter \${\textbackslash}alpha + {\textbackslash}sum{\textasciicircum}n\_1 {\textbackslash}delta\_\{x\_i\}\$, where \${\textbackslash}delta\_x\$ denotes the measure giving mass one to the point \$x\$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space \${\textbackslash}mathscr\{X\}\$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where \${\textbackslash}mathscr\{X\}\$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general \${\textbackslash}mathscr\{X\}\$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that \$P\$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a \$P\$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis \$H\_0\$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter \${\textbackslash}alpha\$ itself a uniform measure on [0, 1], and if we are given a sample of size \$n {\textbackslash}geqq 2\$, the only nontrivial nonrandomized Bayes rule is to reject \$H\_0\$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and ({II}). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.},
	pages = {209--230},
	number = {2},
	journaltitle = {The Annals of Statistics},
	author = {Ferguson, Thomas S.},
	urldate = {2022-11-12},
	date = {1973-03},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Full Text PDF:/home/edovt/Zotero/storage/RPNXFQSB/Ferguson - 1973 - A Bayesian Analysis of Some Nonparametric Problems.pdf:application/pdf;Snapshot:/home/edovt/Zotero/storage/V26TKSCL/1176342360.html:text/html},
}

@article{sethuraman_constructive_1994,
	title = {A Constructive Definition of Dirichlet Priors},
	volume = {4},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/24305538},
	abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
	pages = {639--650},
	number = {2},
	journaltitle = {Statistica Sinica},
	author = {Sethuraman, Jayaram},
	urldate = {2022-11-12},
	date = {1994},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
}

@software{ross_dirichletprocess_2020,
	title = {dirichletprocess: Build Dirichlet Process Objects for Bayesian Modelling},
	rights = {{GPL}-3},
	url = {https://CRAN.R-project.org/package=dirichletprocess},
	shorttitle = {dirichletprocess},
	abstract = {Perform nonparametric Bayesian analysis using Dirichlet processes without the need to program the inference algorithms. Utilise included pre-built models or specify custom models and allow the 'dirichletprocess' package to handle the Markov chain Monte Carlo sampling. Our Dirichlet process objects can act as building blocks for a variety of statistical models including and not limited to: density estimation, clustering and prior distributions in hierarchical models. See Teh, Y. W. (2011) {\textless}https://www.stats.ox.ac.uk/{\textasciitilde}teh/research/npbayes/Teh2010a.pdf{\textgreater}, among many other sources.},
	version = {0.4.0},
	author = {Ross, Gordon J. and Markwick, Dean and Mulder, Kees and Sighinolfi, Giovanni},
	urldate = {2022-11-17},
	date = {2020-06-13},
	keywords = {Bayesian},
}

@article{jara_dppackage_2011,
	title = {{DPpackage}: Bayesian Semi- and Nonparametric Modeling in R},
	volume = {40},
	rights = {Copyright (c) 2009 Alejandro Jara, Timothy Hanson, Fernando A. Quintana, Peter Müller, Gary L. Rosner},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v040.i05},
	doi = {10.18637/jss.v040.i05},
	shorttitle = {{DPpackage}},
	abstract = {Data analysis sometimes requires the relaxation of parametric assumptions in order to gain modeling flexibility and robustness against mis-specification of the probability model. In the Bayesian context, this is accomplished by placing a prior distribution on a function space, such as the space of all probability distributions or the space of all regression functions. Unfortunately, posterior distributions ranging over function spaces are highly complex and hence sampling methods play a key role. This paper provides an introduction to a simple, yet comprehensive, set of programs for the implementation of some Bayesian nonparametric and semiparametric models in R, {DPpackage}. Currently, {DPpackage} includes models for marginal and conditional density estimation, receiver operating characteristic curve analysis, interval-censored data, binary regression data, item response data, longitudinal and clustered data using generalized linear mixed models, and regression data using generalized additive models. The package also contains functions to compute pseudo-Bayes factors for model comparison and for eliciting the precision parameter of the Dirichlet process prior, and a general purpose Metropolis sampling algorithm. To maximize computational efficiency, the actual sampling for each model is carried out using compiled C, C++ or Fortran code.},
	pages = {1--30},
	journaltitle = {Journal of Statistical Software},
	author = {Jara, Alejandro and Hanson, Timothy and Quintana, Fernando A. and Müller, Peter and Rosner, Gary L.},
	urldate = {2022-11-17},
	date = {2011-04-07},
	langid = {english},
}

@article{geman_stochastic_1984,
	title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
	volume = {{PAMI}-6},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field ({MRF}) equivalence, this assignment also determines an {MRF} image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the {MRF}. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an {MRF} with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori ({MAP}) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for {MAP} estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	pages = {721--741},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Geman, Stuart and Geman, Donald},
	date = {1984-11},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Additive noise, Annealing, Bayesian methods, Deformable models, Degradation, Energy states, Gibbs distribution, image restoration, Image restoration, line process, {MAP} estimate, Markov random field, Markov random fields, relaxation, scene modeling, spatial degradation, Stochastic processes, Temperature distribution},
	file = {IEEE Xplore Abstract Record:/home/edovt/Zotero/storage/MIQQDGQV/4767596.html:text/html},
}
