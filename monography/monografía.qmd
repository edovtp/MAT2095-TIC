---
number-sections: true
highlight-style: pygments
format: 
  pdf:
    documentclass: article
    toc: true
    include-in-header: header.tex
    include-before-body: title_page.tex
    toc-title: "Tabla de Contenidos"
    toc-depth: 3
  html:
    toc: true
    code-fold: true
execute: 
  cache: true
crossref: 
  fig-title: Figura
  tbl-title: Tabla
bibliography: references.bib
---

```{julia}
#| include: false
using Random
using StatsBase
using Distributions
using ConjugatePriors
using ElasticArrays
using Plots
using StatsPlots
```

\newpage

\section*{Prefacio}

El presente material, incluyendo la monografía, códigos e ilustraciones, fue confeccionado durante mi Taller de Iniciación Científica (MAT2095), guiado por el profesor Fernando Quintana, durante el segundo semestre del 2022 y verano del 2023. Todo el material se encuentra disponible en el siguiente [repositorio](https://github.com/edovtp/MAT2095-TIC) de Github.

El objetivo principal de este taller fue implementar el algoritmo SIGN [@ni_scalable_2020], que permite aplicar modelos de Mezcla Proceso de Dirichlet (DPM) para bases de datos relativamente grandes. Lo anterior suponía un conocimiento previo de Estadística Bayesiana No Paramétrica, por lo que, en realidad, la mayor parte del trabajo se enfocó en aprender sobre lo anterior, especialmente sobre Procesos de Dirichlet, Mezclas de Procesos de Dirichlet y Modelos de Particiones Aleatorias, incluyendo tanto la teoría como los métodos computacionales disponibles.

En cuanto a la parte computacional, en R existen diferentes paquetes que implementan diferentes modelos Bayesianos no paramétricos, como `DPPackage` [@jara_dppackage_2011] y `dirichletprocess` [@ross_dirichletprocess_2020], así como modelos de particiones aleatorias como `salso` [@dahl_search_2022] y `ppmSuite` [@page_ppmsuite_2022]. Por otro lado, en Julia [@bezanson_julia_2017], que recientemente ha aumentado considerablemente el número de usuarios, no existe mucho desarrollo respecto a los modelos anteriores. Considerando lo anterior, todos los códigos fueron implementados en este lenguaje. Como proyecto a futuro, se podría incluso formar una librería con este material.

La monografía incluye una pequeña introducción tanto a la estadística Bayesiana como al enfoque no paramétrico, para así entender cómo se mezclan ambos conceptos. Luego, se presentan los Procesos de Dirichlet (DP), que es probablemente el punto de partida más común en la estadística Bayesiana no paramétrica. Ya entendiendo estos procesos, pasaremos a una extensión que será el enfoque principal de todo este trabajo, que son los Dirichlet Process Mixture Models (DPM), realizando una pequeña revisión histórica de los métodos de simulación. Finalmente, veremos la conexión entre estos modelos con los Modelos de Particiones Aleatorias, para luego aplicarlo en el contexto de clustering.

\newpage
# Introducción

El material presentado a continuación se enmarca en el área de la Estadística Bayesiana No-Paramétrica, por lo que, en primer lugar, es una buena idea presentar una breve introducción de ambos conceptos. Lo siguiente no pretende ser una introducción exhaustiva, para lo cual existe bibliografía mucho más adecuada.

## Estadística Bayesiana

### Tipos de Incertidumbre

Es común que al presentarnos como estadísticos se nos pregunte acerca de qué es lo que hacemos en nuestro trabajo, ante lo cual solemos responder que nuestro objetivo principal es el de cuantificar la incertidumbre. Explicamos, además, que para lo anterior nos apoyamos sobre la teoría de probabilidades, tomándola como herramienta principal para modelar aquellas incertezas de interés.

Pero, quizás nosotros mismos como estadísticos no hemos reparado acerca de a qué nos referimos exactamente con *incertidumbre*. Esta pregunta es la que nos lleva a las bases mismas de la estadística, así como a entender cómo surgen dos visiones que son diferentes entre sí: la **Estadística frecuentista** (también denominada clásica) y la **Estadística Bayesiana**.

En particular, se distinguen dos tipos de incertidumbres [@ohagan_dicing_2004]. Una de ellas la podemos denominar **incerteza ontólogica** (o aleatoria), mientras que la otra toma el nombre de **incerteza epistemológica** [^1].

[^1]: Es importante mencionar que la ontología es el estudio filosófico del *ser*, mientras que la epistemología es el estudio filosófico del *saber*.

La incerteza ontológica trata acerca de una incerteza que está sujeta a una variabilidad aleatoria innata, que no podemos predecir bajo ninguna cantidad de información. Dentro de los ejemplos de incerteza ontológica se encuentran varios de los ejemplos introductorios a la estadística, como el lanzamiento de un dado o el de ganar la lotería.

Por otro lado, la incerteza epistemológica, tal como lo dice el nombre, es una incerteza acerca de lo que sabemos. La diferencia con la anterior es que en este caso sí podemos obtener información para disminuir, e incluso a veces eliminar, la incerteza. Por ejemplo, podemos tener incerteza acerca de la altura del Costanera Center en Santiago, pero podemos fácilmente buscar en internet esta información, eliminando completamente la incerteza [^2].

[^2]: De hecho, el edificio central tiene una altura de 300 metros.

Así, cuando hablamos de cuantificar la incertidumbre con probabilidades, podemos notar que siempre nos hemos estado refiriendo a incertezas ontológicas. En este sentido, las probabilidades se interpretan como la frecuencia de ocurrencia de un evento, considerando un número infinito de repeticiones. Este es el paradigma *frecuentista* de la estadística.

Ahora, ¿por qué no podemos modelar también las incertezas epistemológicas mediante probabilidades?. Esto es precisamente, de manera justificada, lo que propone el paradigma *Bayesiano*. En este caso ya no podemos interpretar las probabilidades como frecuencias, si no que como una *medida racional de incerteza*, lo cual normalmente dependerá de cada persona.

### Modelos Bayesianos

Como vimos, el paradigma Bayesiano se basa en la idea de probabilidad subjetiva, donde estas cantidades reflejan el grado de creencia que un individuo tiene con respecto a eventos particulares.

En cuanto al modelamiento, estas creencias son plasmadas en una distribución a priori de los parámetros de interés, $\pi(\theta)$ [^3], denominada simplemente **priori** de aquí en adelante. Además, debemos definir la verosimilitud de nuestros datos, $p(\mathbf{y}|\theta)$, que refleja justamente qué tan verosímiles son nuestros datos observados, dado un cierto valor de $\theta$. Finalmente, ambas componentes son utilizadas para definir un modelo conjunto tanto de cantidades observables como no observables, esto es,

[^3]: $\theta$ puede ser multidimensional.

$$
p(\mathbf{y}, \theta) = p(\mathbf{y}|\theta)\pi(\theta)
$$

Luego, a la luz de nueva información, se actualiza nuestra creencia a priori, mediante el teorema de Bayes, obteniendo entonces la distribución a posteriori que llamaremos simplemente **posteriori** en lo que sigue.

$$
\begin{aligned}
  \pi(\theta|\mathbf{y}) &= \frac{f(\mathbf{y}|\theta)\pi(\theta)}{\int_{\Theta}f(\mathbf{y}|\theta)\pi(\theta)d\theta} \\
  &\propto f(\mathbf{y}|\theta)\pi(\theta)
\end{aligned}
$$ {#eq-posterior}

Por otro lado, es posible que también tengamos interés en la predicción de valores observables a futuro. Esto se obtiene fácilmente marginalizando la incerteza con respecto a las cantidades no observables, i.e.

$$
\begin{aligned}
  p(y_{n+1}|\mathbf{y}) &= \int p(y_{n+1}, \theta|\mathbf{y})d\theta \\
  &=\int p(y_{n+1}|\theta)\pi(\theta|\mathbf{y})d\theta
\end{aligned}
$$ {#eq-posterior-predictive}

donde en la segunda ecuación realizamos el supuesto de independencia condicional entre las observaciones, dado los valores del vector de parámetros $\theta$. Ambos resultados, ([-@eq-posterior]) y ([-@eq-posterior-predictive]), podemos considerarlos como los productos principales para ser utilizados en la inferencia estadística mediante este paradigma.

\newpage
```{=tex}
\begin{ejemplo}[Modelo Normal-Normal]
Para aterrizar los conceptos anteriores, consideremos el siguiente modelo:
$$
\begin{aligned}
  y_1, ..., y_n | \theta \overset{i.i.d.}&{\sim} \text{N}(\theta, \sigma^2) \\
  \theta &\sim \text{N}(\mu_0, \sigma_0^2)
\end{aligned}
$$

donde $\sigma^2$ es conocido. Utilizando la ecuación (1) es fácil mostrar que

$$
  \theta | y_1, ..., y_n \sim N(\mu_n, \sigma_n^2)
$$

donde 

$$
  \mu_n = \frac{(1/\sigma_0^2)}{1/\sigma_0^2 + n/\sigma^2}\mu_0 + \frac{(n/\sigma^2)}{1/\sigma_0^2 + n/\sigma^2}\bar{y}
$$

y

$$
  \sigma_n^2 = \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}
$$

Por otro lado, también es posible mostrar que, utilizando la ecuación (2),

$$
  y_{n+1}|\mathbf{y} \sim \text{N}(\mu_n, \sigma^2 + \sigma_n^2)
$$
\end{ejemplo}
```

El ejemplo anterior muestra una de las características principales de la estadística Bayesiana, que es la combinación de la información a priori con la información de los datos en los resultados a posteriori. En particular, notamos que:

* La media a posteriori corresponde a un promedio ponderado de la media a priori y el promedio muestral.
* La varianza a posteriori es un promedio armónico de la varianza a priori y la varianza del promedio muestral.

Además, en el ejemplo anterior obtuvimos un resultado bastante conveniente. Al considerar una función de verosimilitud Normal, así como una priori Normal para $\theta$, obtuvimos que la posteriori sigue siendo una distribución Normal. Este tipo de modelos se denominan **conjugados** y serán importantes en los siguientes capítulos. Presentamos la definición formal a continuación.

\newpage
```{=tex}
\begin{definicion}[Priori conjugada]
  Decimos que una clase $\mathcal{P}$ de distribuciones a priori es \textbf{conjugada} para la verosimilitud $f(\mathbf{y}|\theta)$ si
  $$
    \pi(\theta) \in \mathcal{P} \implies \pi(\theta|\mathbf{y}) \in \mathcal{P}
  $$
  esto es, la distribución a posteriori de $\theta$ sigue teniendo la misma distribución que la priori.
\end{definicion}
```

Los modelos conjugados son convenientes ya que obtenemos resultados analíticamente tractables, siempre y cuando esta clase sea analíticamente tractable, así como generalmente intuitivos. Ahora, nada nos debe restringir a ocupar modelos conjugados. En particular, vemos que, en un principio, tanto para la verosimilitud como para la priori podemos ocupar cualquier distribución de probabilidad, que deberán ser elegidas de acorde al problema.

Lo anterior provocó uno de los cuellos de botella más importantes de la estadística Bayesiana, que frenó su amplio uso en la práctica, donde los modelos necesitaban ser cada vez más realísticos. Principalmente, el problema radica en los cálculos computacionales necesarios para realizar la inferencia a posteriori, que incluye tanto la evaluación de ([-@eq-posterior]) y ([-@eq-posterior-predictive]), las cuales no siempre pueden ser obtenidas de forma analítica.

Para resolver parcialmente lo anterior se ha recurrido principalmente a métodos de simulación, utilizando muestras a posteriori para evaluar diferentes cantidades de interés, como momentos, cuantiles, probabilidades, etc. Entre estos métodos computacionales se encuentran Rejection Sampling, Importance Sampling y métodos basados en cadenas de Markov como Slice sampling, Gibbs Sampling, Metropolis-Hastings, Hamiltonian Monte Carlo (HMC) y No-U-Turn Sampler (NUTS), Sequential Monte Carlo, entre otros. Para una introducción a cada uno de los métodos anteriores, ver [@gelman_bayesian_2014].

No solo estos modelos se han ido perfeccionando con el tiempo, si no que también se han desarrollado diferentes softwares que permiten definir nuestros modelos para realizar la simulación a posteriori, sin tener que uno programar toda la maquinaria necesaria. Algunos ejemplos son los lenguajes probabilísticos BUGS, JAGS, Stan, PyMC3 y Turing. Además, existen librerías escritas sobre estos lenguajes probabilísticos, que ayudan aún más a que personas de otras áreas puedan fácilmente ajustar sus modelos.

\newpage
## Estadística No-Paramétrica

Normalmente, en estadística asumimos

$$
y_1,..., y_n | G \overset{i.i.d.}{\sim} G
$$

Suponemos que la densidad de $G$, $g$, pertenece a 

$$
\mathcal{G} = \{g_\theta\colon \theta \in \Theta \subset \mathbb{R}^p\}
$$

* Ejemplo

* Figura

::: {#fig-np layout-ncol=2}
![](figures/NP%20-%20Example%201.jpg)

![](figures/NP%20-%20Example%202.jpg)

Necesidad de métodos flexibles
:::

* Nos gustaría ir un poco más allá: estimación de densidades (figura) y regresión

## Estadística Bayesiana No-Paramétrica

\newpage
# Procesos de Dirichlet

Consideremos, en primer lugar, el problema de estimación de densidades. Específicamente, consideremos una muestra aleatoria $y_1, ..., y_n$ de una medida de probabilidad $G$ no especificada, denotado por $y_1, ..., y_n | G \overset{i.i.d.}{\sim} G$.

Considerando el paradigma Bayesiano, lo anterior debe ser completado con una medida de probabilidad sobre $G$, esto es, sobre el espacio de las medidas de probabilidad. Una de estas opciones es la del Proceso de Dirichlet [@ferguson_bayesian_1973], que probablemente sea uno de los puntos de partida más usuales a los modelos no paramétricos, y que tomará una gran importancia en los siguientes capítulos.

## Definición

Presentamos a continuación la definición formal de estos procesos.

```{=tex}
% Definición Proceso de Dirichlet
\begin{definicion}[(Ferguson, 1973)]
  Sea $M>0$ y $G_0$ una medida de probabilidad definida sobre $S$. Un \textbf{Proceso de Dirichlet (DP)} de parámetros $(M, G_0)$, denotado por $\text{DP}(M, G_0)$, es una medida de probabilidad aleatoria $G$ definida en $S$ que asigna probabilidad $G(B)$ a todo conjunto medible $B$ tal que, para toda partición medible finita $\{B_1, ..., B_k\}$ de $S$, la distribución conjunta del vector $(G(B_1), ..., G(B_k))$ es Dirichlet con parámetros

$$
(M G_0(B_1), ..., M G_0(B_k))
$$

Los parámetros $G_0$ y $M$ se denominan la \textbf{medida de centralización} y la \textbf{precisión}, respectivamente. También se suele denominar $M G_0$ como la \textbf{medida base}.
\end{definicion}
```

```{=tex}
\begin{nota}
  En la mayoría de la literatura se suele utilizar $\alpha$ en vez de M. Decidí en este caso utilizar la segunda opción para así tener disponible la nomenclatura $\alpha$ para parámetros de otras distribuciones, como por ejemplo la Gamma.
\end{nota}
```

En su artículo, Ferguson muestra que $G$ existe para todo $G_0$. Además, señala algunas propiedades de los DP, algunas de las cuales se deducen directamente de la definición del proceso al considerar la partición $\{B, B^c\}$.

\newpage
```{=tex}
\begin{propiedad}[(Propiedades Proceso de Dirichlet)]
  Sea $G \sim \text{DP}(M, G_0$), $B, B_1$ y $B_2$ conjuntos medibles, con $B_1 \cap B_2 = \emptyset$. Luego,
  \begin{itemize}
    \item El soporte de $G$ coincide con el de $G_0$. Esto es,
    \begin{equation*}
      G_0(B) = 0 \implies P(G(B) = 0) = 1
    \end{equation*}
    y
    \begin{equation*}
      \quad G_0(B) > 0 \implies P(G(B) > 0) = 1 
    \end{equation*}
    \item $\text{E}(G(B)) = G_0(B)$
    \item $\text{Var}(G(B)) = \frac{G_0(B)(1 - G_0(B)}{1 + M}$
    \item $\text{Cov}(G(B_1), G(B_2)) = \frac{-G_0(B_1)G_0(B_2)}{1 + M}$
  \end{itemize}
\end{propiedad}
```

La segunda propiedad nos muestra la razón por la que $G_0$ recibe el nombre de *medida de centralización*. Por otro lado, la tercera propiedad nos da la intuición por la que el parámetro M recibe el nombre de *parámetro de precisión*, al controlar el grado de concentración en la media $G_0(B)$. Por último, la última propiedad nos entrega una característica bastante interesante, y es que la covarianza entre dos conjuntos cualesquiera es siempre negativa. Lo anterior puede ser una característica no deseada, donde esperaríamos que exista una alta correlación entre conjuntos $B_1$ y $B_2$ cercanos entre sí, pero aquellas extensiones están fuera del alcance de este trabajo.

```{=tex}
\begin{nota}
  Algunos se preguntarán, como yo lo hice al empezar a estudiar estos temas, el por qué lo anterior se denomina un \textbf{proceso}. La razón es bastante sencilla, y es que el DP es un proceso estocástico que, en vez de estar indexado por índices comunes como el tiempo o coordenadas geográficas, está indexado por todos los conjuntos medibles, esto es, para cada conjunto medible $B$ tenemos la variable aleatoria $G(B)$.
\end{nota}
```

Una propiedad muy importante, que será central en el transcurso de esta monografía, es que $G$ es casi-seguramente discreta. Este resultado nos dice que $G$ se puede escribir como una suma ponderada de masas puntuales (también denominados *átomos*). Esto es,

$$
G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot)
$$

donde $\sum_{h=1}^\infty w_h = 1$ y $\delta_{x}(\cdot)$ denota la medida de Dirac en $x$, i.e. $\delta_x(A) = 1$ si $x\in A$ y $\delta_x(A) = 0$ en caso contrario. En la Figura 2 se ilustra gráficamente un Proceso de Dirichlet. A la izquierda se ilustran los átomos con puntos morados, donde los largos indican el peso que aporta cada uno. A la derecha se muestra cómo se calcularía la probabilidad para un cierto conjunto medible $B$, que es simplemente tomar la suma de las masas de los átomos que se encuentran dentro de este conjunto.

::: {#fig-dp layout-ncol=2}
![](figures/DP%20-%20Definition.jpg)

![](figures/DP%20-%20Definition%202.jpg)

Naturaleza discreta del Proceso de Dirichlet
:::

Por último, Ferguson tambien demuestra que un DP es conjugado para una muestra i.i.d. de esta distribución, donde para la medida de centralización se considera un promedio ponderado entre la medida a priori $G_0$ y la función de distribución empírica de los datos, mientras que la precisión aumenta con el número de observaciones.

```{=tex}
% Posteriori Proceso de Dirichlet
\begin{proposicion}[(Ferguson, 1973)]
  Sea $y_1, ..., y_n | G \overset{i.i.d}{\sim} G$ y $G \sim \text{DP}(M, G_0)$. Luego,
  \begin{equation*}
    G | y_1, ..., y_n \sim \text{DP}\left(M + n, \frac{M G_0 + n \hat{f}_n}{M + n}\right)
  \end{equation*}
  donde $\hat{f}_n$ es la distribución empírica obtenida a partir de los datos, i.e.
  \begin{equation*}
    \hat{f}_n(\cdot) = \frac{1}{n}\sum_{i=1}^n \delta_{y_i}(\cdot)
  \end{equation*}
\end{proposicion}
```

Ahora, todo lo anterior aún no nos dice mucho acerca de como trabajar con esta distribución, ya que de momento sólo sabemos que existen tales procesos. En la práctica, se ha trabajado principalmente de dos formas. La primera es marginalizando la medida de probabilidad aleatoria $G$, esto es, trabajar directamente con

$$
p(y_1, ..., y_n) = \int p(y_1, ..., y_n|G) d\pi(G)
$$

La otra forma es considerar la construcción de un DP mediante una representación basada en cortar una varilla de largo unitario, de manera sucesiva e indefinida, denominada *Stick-Breaking*.

## Construcción de Procesos de Dirichlet

### Urnas de Pólya o Proceso del Restaurante Chino

Una de las formas de poder trabajar con un Proceso de Dirichlet es, irónicamente, no trabajar con él. En probabilidades esto lo logramos marginalizando con respecto a la medida de la variable que no es de interés.

Considerando una muestra aleatoria $y_1, ..., y_n|G \sim G$, Blackwell y MacQueen  [-@blackwell_ferguson_1973] formulan una representación de la densidad marginal $p(y_1, ..., y_n)$ mediante una construcción por Urnas de Pólya. En particular, sabemos que la densidad marginal conjunta podemos expresarla como

$$
p(y_1, ..., y_n) = p(y_1)\prod_{i=2}^n p(y_i|y_1, ..., y_{i-1})
$$

y lo que muestran ambos autores es que $p(y_1)$ es la densidad de $G_0$ y que, para $i = 2, ..., n$,

$$
p(y_i|y_1, ..., y_{i-1}) = \frac{1}{M + i - 1}\sum_{h=1}^{i-1}\delta_{y_h}(y_i) + \frac{M}{M + i - 1}G_0(y_i)
$${#eq-dp-marginal}

Este resultado puede ser interpretado de la siguiente manera, razón por la cuál se asocia a Urnas de Pólya: consideremos una especie de generador de colores $G_0$ y una urna en la cuál iremos depositando pelotas de diferentes colores. Partimos nuestro proceso generando un color y depositando una pelota en la urna de ese mismo color. Luego, seguimos el proceso para $i = 1, 2, ...,$ como:

* Con probabilidad $M/(M + i - 1)$ generamos un nuevo color de $G_0$ y depositamos en la urna una pelota de ese color.
* En caso contrario, sacamos una pelota al azar de la urna y devolvemos una pelota extra del mismo color.

Lo que muestran Blackwell y MacQueen es que, siguiendo este proceso de forma infinita, el proceso converge a un proceso de Dirichlet, por lo que las densidades marginales finitas coinciden y están dadas por la ecuación ([-@eq-dp-marginal]).

```{=tex}
\begin{nota}
  Esta construcción también recibe el nombre del Proceso del Restaurante Chino (CRP). En este caso, se considera un restaurante de mesas infinitas y que $G_0$ es un generador de comidas. De manera sucesiva entran clientes y los sentamos en una mesa vacía con probabilidad $M/(M + i - 1)$, sirviéndoles un nuevo plato de $G_0$ o, en caso contrario, se elige a otro cliente al azar, sentando al nuevo cliente en la misma mesa, y sirviéndole el mismo plato.
\end{nota}
```

De ([-@eq-dp-marginal]) también se pueden observar tres cosas importantes:

* Dada la intercambiabilidad de $y_1, ..., y_n$, las distribuciones condicionales completas $p(y_i|y_{-i})$ toman la misma forma que para $i = n$ en ([-@eq-dp-marginal]). Estas distribuciones son la pieza clave para realizar simulación a posteriori mediante muestreo de Gibbs.
* La distribución a priori predictiva toma la misma forma que para $i = n + 1$.
* Las condicionales ([-@eq-dp-marginal]) se pueden simplificar considerando solo los valores iguales de $y_1, ..., y_{i-1}$. Sean $y^{*}_1, ..., y^{*}_k$ los valores únicos y $n_j$, $j=1,...,k$ el número de veces que se repite cada valor. Luego podemos escribir la distribución marginal de forma equivalente como

$$
p(y_i|y_1, ..., y_{i-1}) = \frac{n_j}{M + i - 1}\sum_{j=1}^{k}\delta_{y_j^{*}}(y_i) + \frac{M}{M + i - 1}G_0(y_i)
$$

```{=tex}
\begin{ejemplo}[Urnas de Pólya]
  Para ilustrar los resultados anteriores, en la Figura 3 se presentan muestras aleatorias de tamaño 100 de la distribución marginal de un proceso de Dirichlet, considerando una medida de centralización Gamma(6, 1) y diferentes valores de $M$. \\

  Para replicar esta figura, ver el código \texttt{02\_DpData.jl}.
\end{ejemplo}
```

::: {#fig-dp-urn}
![](figures/DP%20-%20Urn.png)

Muestra simulada directamente de la distribución marginal de un proceso de Dirichlet. Es posible ver que, a medida que $M$ aumenta, el histograma se va pareciendo cada vez más a $G_0$, representada en azul. 
:::

### Stick-Breaking

La construcción anterior entrega la forma de trabajar con procesos de Dirichlet de forma indirecta, por lo que es natural preguntarnos si existirá alguna opción directa. En esta línea, en los años 90 se obtiene un resultado que permite trabajar directamente con estos procesos [@sethuraman_constructive_1994]. Es importante poner énfasis en la fecha de este resultado, ya que el anterior que vimos se dio en 1973, lo cual explica por qué mucho del desarrollo en estos procesos utilizaba la construcción previa.

\newpage
En particular, se demuestra el siguiente teorema.

```{=tex}
\begin{theo}[(Sethuraman, 1994)]
  Sea $w_h = \upsilon_h \prod_{l<h} (1 - \upsilon_l)$ con $\upsilon_h \overset{i.i.d.}{\sim} \text{Beta}(1, M)$ y $m_h \overset{i.i.d.}{\sim} G_0$, donde $(\upsilon_h)$ y $(m_h)$ son independientes entre sí. Luego,

  \begin{equation*}
    G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot) \sim \text{DP}(M, G_0)
  \end{equation*}
\end{theo}
```

Esta construcción recibe el nombre de \textbf{stick-breaking}, lo cual puede verse al pensar en los $\upsilon_h$ como las proporciones que se van eliminando de una varilla que inicialmente es de largo 1, mientras que los $w_h$ serían la proporción del total de la varilla que toma como peso para el átomo $m_h$. La Figura 4 presenta una pequeña ilustración de esta construcción.

Ahora, la construcción anterior sigue teniendo un pequeño problema, y es que claramente no podemos repetir el proceso de obtención de pesos y átomos de manera infinita. Una de las formas de evitar este problema es simplemente truncar la representación hasta un número $H$ fijo, fijando $\upsilon_H = 1$, denominado un \textbf{DP finito} [@ishwaran_markov_2000], u obtener los pesos $w_h$ hasta cubrir un cierto número fijo cercano a 1 de probabilidad, denominado un $\boldsymbol\epsilon$-\textbf{DP} [@muliere_approximating_1998].

::: {#fig-stickbreaking}
![](figures/DP%20-%20Stick%20Breaking.jpg)

Ilustación del proceso de Stick-Breaking. (a) Inicialmente se parte con una varilla de largo 1. (b) Se toma una proporción $\upsilon_1$ del total para el primer átomo, dándole un peso $w_1$. (c) De la varilla restante, se toma una proporción $\upsilon_2$ para el segundo átomo.
:::

```{=tex}
\begin{ejemplo}[Stick-Breaking]
  Para ilustrar esta construcción, en la Figura 4 se presentan simulaciones de procesos de Dirichlet, considerando una medida de centralización Gamma(6, 1) y diferentes valores de $M$. Específicamente, se presentan las simulaciones de $G((0, x))$ (i.e. la función de distribución acumulada).

  Para replicar esta figura, ver el código \texttt{01\_Dp.jl}.
\end{ejemplo}
```

::: {#fig-dp-sb-eg}
![](figures/DP%20-%20Stick%20Breaking%20eg.png)

Muestras de un proceso de Dirichlet, representadas en gris, junto a la función de distribución acumulada de $G_0$, representada en rojo. Es posible ver cómo las simulaciones se concentran alrededor de $G_0$ y que, a medida que $M$ aumenta, la concentración es mayor, lo que concuerda con las propiedades vistas anteriormente.
:::


```{=tex}
\begin{nota}
  En este punto del trabajo fue donde decidí cambiarme de R a Julia, ya que las simulaciones anteriores tomaban demasiado tiempo. Utilizando Julia obtuve una mejoría en rapidez de casi 100 veces.
\end{nota}
```

## Otros resultados y extensiones

En las secciones anteriores vimos que, dada la naturaleza discreta del proceso de Dirichlet, existe una probabilidad positiva de repeticiones en los valores $y_1, ..., y_n$, los que denotamos por $y_1^*, ..., y_k^*$, $k \leq n$. En esta sección veremos algunos resultados con respecto a la distribución y comportamiento de $k$, el número de valores únicos.

En primer lugar, tenemos que el comportamiento asintótico de $k$ se comporta como $M \log (n)$ cuando $n \to \infty$ [@korwar_contributions_1973]. Por otro lado, Antoniak [-@antoniak_mixtures_1974] entrega la distribución a priori de $k$, dada por

$$
p(k|M, n) = c_n(k) n! M^k \frac{\Gamma(M)}{\Gamma(M + n)}
$$

con $c_n(k) = P(k|M = 1, n)$ (este es el resultado que entrega Escobar y West, pero revisando el paper de Antoniak no logro ver que sean iguales). Además, muestra que:

$$
E(k|M, n) = \sum_{i=1}^n \frac{M}{M + i - 1} \approx M \left(\log (n/M + 1)\right)
$$

La Figura 6 ilustra estos resultados. Para la simulación se utilizaron 15 muestras para tamaños de muestra que van desde 1000 hasta 50000.

::: {#fig-stickbreaking}
![](figures/DP%20-%20AKH.png)

Ilustación de los resultados de Antoniak, Korwar y Hollander, junto con estimaciones empíricas de $E(k)$ mediante simulación. Vemos que efectivamente la aproximación dada por 
Antoniak es bastante certera.
:::

\newpage
# Dirichlet Process Mixture Models

## Introducción

Obtener distribuciones discretas puede no ser adecuado para diferentes problemas. Nos interesa entonces extender los procesos de Dirichlet para formular modelos que los utilicen y reflejen realmente el problema considerado. En lo que sigue, utilizaremos principalmente la notación de Neal (2000), mezclándola en parte con la del libro *Bayesian Nonparametric Data Analysis* [@muller_bayesian_2015].

Una opción es usar estas medidas de probabilidad aleatoria como la *mixing distribution* en un modelo de mezcla, esto es,

$$
f_G(y) = \int f_\theta(y)dG(\theta)
$$

Equivalentemente, lo anterior puede ser escrito a través de un modelo jerárquico, introduciendo variables latentes $\theta_i$ para cada una de las observaciones. Esto es,

$$
\begin{aligned}
y_i|\theta_i &\overset{ind.}{\sim} F(\theta_i) \\
\theta_i|G &\overset{i.i.d.}{\sim} G \\
G &\sim \text{DP}(\alpha, G_0)
\end{aligned}
$$

De lo anterior sale una característica de este modelo que será clave en lo que sigue, particularmente en el contexto de clustering y es lo que vimos en la sección anterior con respecto a la distribución marginal de los $\theta_i$. En particular,

$$
\theta_i | \theta_{-i}, y_i \sim \sum_{j\neq i}q_{i,j}\delta(\theta_j) + r_i H_i
$$

donde,

$$
\begin{aligned}
  q_{i,j} &= bF(y_i, \theta_j) \\
  r_i &= b\alpha \int F(y_i, \theta)dG_0(\theta)
\end{aligned}
$$

donde $b$ es tal que $\sum_{j\neq i}q_{i,j} + r_i = 1$ y $H_i$ es la posteriori que se obtiene al considerar un modelo con $G_0$ como priori y una única observación de la verosimilitud $F(y_i, \theta_i)$.

* Resultado $\alpha$: depende solo del número de valores únicos en $\theta$.
* Resultado $\eta$: proporcional a su priori multiplicada por $\prod_{c} G_0(\theta_i)$.

**Resultado de Antoniak**

De manera más general podemos incluir inferencia sobre $\alpha$ e hiperparámetros de la medida de centro $G_0$. Procedemos entonces con el siguiente modelo:


$$
\begin{aligned}
  y_i|\theta_i \overset{ind.}&{\sim} F(\theta_i) \\
  \theta_i | G \overset{i.i.d.}&{\sim} G \\
  G &\sim \text{DP}(\alpha, G_\eta) \\
  (\alpha, \eta) &\sim \pi
\end{aligned}
$${#eq-dpm}


## Simulación a posteriori

Radford Neal presenta una revisión histórica de los algoritmos de simulación para el modelo ([-@eq-dpm]), los cuales se basan principalmente en Gibbs Sampling [@geman_stochastic_1984].

En particular, nuestra meta principal es poder obtener muestras de la distribución a posteriori de $\theta = (\theta_1, ..., \theta_n)$. Estas muestras nos permiten entonces evaluar la cantidad de interés en estimación de densidades, que es (* LINK O ECUACIÓN DE LA POSTERIORI PREDICTIVA). En particular,

$$
\begin{aligned}
  p(y_{n+1}|\mathbf{y}) &= \int F(\theta_{n+1})dP(\theta_{n + 1}|\mathbf{y}) \\
  &= \int F(\theta_{n+1})p(\theta_{n+1}|\theta)dP(\theta|\mathbf{y}) \\
  &\approx (1 / T) \sum_{t=1}^T F(\theta_{n+1}^{(t)})
\end{aligned}
$$

donde $\theta_{n+1}^{(t)}$, $t=1,...,T$, corresponden a muestras de $\theta_{n+1}|\theta$, dadas por (* INSERTAR ECUACIÓN DE LA POSTERIORI PREDICTIVA).

La clave entonces es poder simular de la posteriori. El muestreo de Gibbs nos dice que una forma de realizar lo anterior es muestrear de las distribuciones condicionales completas a posteriorio, lo cual está dado por la (* LINK A ECUACIÓN ANTERIOR). Notamos entonces que hay dos problemas claves:

* Muestrear de la distribución $H_i$, $i = 1,...,n$
* Calcular la integral en $r_i$

A continuación, veremos algunos casos para mitigar ambos problemas, realizando una revisión histórica de los diferentes algoritmos basados en muestreo de Gibbs, que culmina en lo que se denomina el *Algoritmo 8* de Neal, que permite realizar simulación en el modelo general dado en ([@eq-dpm]).

### Caso conjugado

#### Escobar & West (1995)

```{=tex}
\begin{ejemplo}[(Datos de Galaxias)]
Modelo propuesto por Escobar y West en el paper del 1995

To finalize the article, the authors present a final extension of the previous algorithm that now includes learning about the precision parameter $\alpha$.

This final model is represented as,

$$
\begin{aligned}
  Y_i | \pi_i &\overset{ind.}{\sim} \text{N}(\mu_i, V_i), \quad i = 1, ..., n \\
  \pi_1, ..., \pi_n &\overset{i.i.d.}{\sim} G  \\
  G &\sim DP(\alpha, G_0) \\
  G_0 &= N-\Gamma^{-1}(m, 1/\tau, s/2, S/2) \\
  \tau &\sim \Gamma^{-1}(w/2, W/2) \\
  m &\sim N(0, A), \quad A \to \infty \\
  \alpha & \sim \Gamma(a, b) 
\end{aligned}
$$

It follows that,

* The full conditional of $\alpha$ is given by,

$$
\alpha | \pi, m, \tau, D_n \equiv \alpha | \eta, k \sim \pi_\eta \Gamma(a + k, b - \log \eta) + (1 - \pi_\eta)\Gamma(a + k - 1, b - \log \eta)
$$

where $\pi_\eta/(1 - \pi_\eta) = (a + k - 1)/[n (b - \log \eta)]$. Here we introduced an auxiliary variable $\eta$ that satisfies

$$
\eta | \alpha, k \sim \text{Beta}(\alpha + 1, n)
$$

* The full conditional of $m$ is given by,

$$
m | \tau, \pi, \alpha, D_n \equiv m | \pi, \tau \sim N\Big[ x\bar{V}\sum(V_j^*)^{-1}\mu_j^*\,;\, x\tau\bar{V}\Big]
$$

  where $x = A/(A + \tau \bar{V})$ and $\bar{V}^{-1} = \sum(V_j^*)^{-1}$.

* The full conditional of $\tau$ is given by,

$$
\tau | \pi, m, \alpha, D_n \equiv \tau | \pi, m \sim \Gamma^{-1}((w + k)/2, (W + K)/2)
$$

  where $K = \sum_{j=1}^k (\mu_j^* - m)^2/V_j^*$.

* The full conditionals of $\pi$ are given by,

$$
\pi_i|\pi^{(i)}, m, \tau, \alpha, D_n \sim q_0G_i(\pi_i) + \sum_{j\neq i}
q_j \delta_{\pi_j}(\pi_i)
$$

  where $G_i(\pi_i) \equiv N-\Gamma^{-1}(x_i, 1/X; (1 + s)/2, S_i/2)$ and 

$$
    \begin{aligned}
    q_0 &\propto \alpha c(s)[1 + (y_i - m)^2/(sM)]^{-(1 + s)/2}/M^{1/2} & \propto \alpha \cdot t_s(m, \sqrt{M})\\
    q_j &\propto \exp[-(y_i - \mu_j)^2/(2V_j)](2V_j)^{-1/2} &\propto N(\mu_j, V_j) \\
    \sum_{j=0, j\neq i}^n q_j &= 1 
    \end{aligned}
$$

  with

  -   $x_i = (m + \tau y_i)/(1 + \tau)$
  
  -   $X = \tau/(1 + \tau)$
  
  -   $S_i = S + (y_i - m)^2/(1 + \tau)$
  
  -   $M = (1 + \tau)S/s$
  
  -   $c(s) = \Gamma((1 + s)/2)\Gamma(s/2)^{-1}s^{-1/2}$

Thus, the algorithm proceeds as follows:

1. Sample initial values in the following way:
    * Sample $\alpha \sim \Gamma(a, b)$
    * Sample $\tau \sim \Gamma^{-1}(w/2, W/2)$
    * Sample $m \sim N(0, 1)$
    * Sample $\pi$ given $m, \tau$
2. For $t = 1, ..., N$:
    * Sample $\eta_{(t)}|\alpha_{(t-1)}, k_{(t-1)}$
    * Sample $\alpha_{(t)} | \eta_{(t)}, k_{(t-1)}$
    * Sample $m_{(t)} | \pi_{(t-1)}, \tau_{(t-1)}$
    * Sample $\tau_{(t)} | \pi_{(t-1)}, m_{(t)}$
    * Sample $\pi_{i, (t)} | \pi^{(i)}_{(t-1)}, m_{(t)}, \tau_{(t)}, \alpha_{(t)}, D_n$ from its full conditional. Be aware that $\pi^{(i)}_{(t-1)}$ may contain already updated values of the form $\pi_{1, (t)}, ..., \pi_{i-1, (t)}, \pi_{i+1, (t-1)}, ..., \pi_{n, (t-1)}$.
\end{ejemplo}
```

```{julia}
#| label: fig-galaxias1
#| echo: false
#| fig-cap: Velocidades de galaxias

# include("code/c3_f.jl")

# velocities = [9172, 9558, 10406, 18419, 18927, 19330, 19440, 19541,
#     19846, 19914, 19989, 20179, 20221, 20795, 20875, 21492,
#     21921, 22209, 22314, 22746, 22914, 23263, 23542, 23711,
#     24289, 24990, 26995, 34279, 9350, 9775, 16084, 18552,
#     19052, 19343, 19473, 19547, 19856, 19918, 20166, 20196,
#     20415, 20821, 20986, 21701, 21960, 22242, 22374, 22747,
#     23206, 23484, 23666, 24129, 24366, 25633, 32065, 9483,
#     10227, 16170, 18600, 19070, 19349, 19529, 19663, 19863,
#     19973, 20175, 20215, 20629, 20846, 21137, 21814, 22185,
#     22249, 22495, 22888, 23241, 23538, 23706, 24285, 24717,
#     26960, 32789] ./ 1000;

# histogram(velocities, bins=1:40, label="")
```

```{julia}
#| label: fig-pos-ew
#| echo: false
#| fig-cap: Densidades estimadas a posteriori
#| fig-subcap:
#|   - Predictiva a posteriori
#|   - $\alpha$|D
#| layout-ncol: 2
#| layout-nrow: 1

# a, b, A, w, W, s, S = 2, 4, 1000, 1, 100, 4, 2;
# prior_par = (a, b, A, w, W, s, S);

# Random.seed!(219);
# N = 4000;
# warmup = 2000;
# g_eta, g_alpha, g_mt, g_pi = tic_dpm_ew(velocities, prior_par, N, warmup);

# n = length(velocities)
# function cond_dens(y)
#     s1 = [
#         g_alpha[i] * pdf(TDist(s), (y - g_mt[i, 1]) / sqrt(1 + g_mt[i, 2] * S / s)) /
#         sqrt(1 + g_mt[i, 2] * S / s) for i in 1:(N-warmup)
#     ]

#     s2 = [
#         sum(map(x -> pdf(Normal(x[1], sqrt(x[2])), y), eachrow(sample)))
#         for sample in eachslice(g_pi, dims=1)
#     ]

#     dens = (s1 .+ s2) ./ (g_alpha .+ n)

#     return mean(dens)
# end

# y_grid = range(8, 40, length=500);
# dens_est = [cond_dens(y) for y in y_grid];

# histogram(velocities, bins=1:40, label="", normalize=true);
# display(plot!(y_grid, dens_est, label="Densidad estimada", linewidth=2.5))


# function cond_dens(alpha)
#     function a_dist(i, alpha)
#         eta = g_eta[i]

#         unique_pi = unique(g_pi[i, :, :], dims=1)
#         k = size(unique_pi)[1]
#         odds_w = (a + k - 1) / (n * (b - log(eta)))
#         weight = odds_w / (1 + odds_w)

#         weight * pdf(Gamma(a + k, 1 / (b - log(eta))), alpha) +
#         (1 - weight) * pdf(Gamma(a + k - 1, 1 / (b - log(eta))), alpha)
#     end

#     dens = [a_dist(i, alpha) for i in 1:(N-warmup)]
#     return mean(dens)
# end

# alpha_grid = range(0, 3, length=500);
# dens_est = [cond_dens(alpha) for alpha in alpha_grid]
# plot(alpha_grid, dens_est, label="Estimated density", linewidth=3);
# display(plot!(alpha_grid, pdf(Gamma(a, 1 / b), alpha_grid), label="Prior density"))
```

#### Problema de "sticky-clusters"

Problema: sticky clusters

```{julia}
#| label: fig-sticky-clusters
#| echo: false
#| fig-cap: Simulación a post de un parámetro. Vemos que se queda pegado

# aux_theta = g_pi[1:end, 1, 1]
# plot(1:500, aux_theta[1:500])
```

Bush & MacEachern, separación de valores.

**Figura de cómo recuperar valores utilizando las indicadoras de clusters**





### Caso no-conjugado

* No-gaps de MacEachern & Muller
* Algoritmo 8 de Neal


### Otras opciones

Inferencia variacional, DP $\epsilon$-finito.


\newpage
# Modelos de Particiones Aleatorias y Clustering

\newpage
# Aplicación: Modelo CAPM

* El modelo de valorización de activos financieros (CAPM) fue desarrollado en los años 60 de forma independiente por Jack Treynor, William Sharpe, John Linter y Jan Mossin.

* El modelo propone 

$$
\text{E}(R) = r_f + \beta(\text{E}(R_m) - r_f)
$$

donde R es el retorno del activo, $r_f$ es la tasa de retorno libre de riesgo, $R_m$ es el retorno del mercado y $\beta$ es el riesgo sistemático del activo bajo estudio.

* Normalmente se considera el modelo de regresión

$$
Y_j \equiv r_j - r_{fj} = \alpha + \beta(r_{mj} - r_{fj}) + \varepsilon_j, \quad j = 1,...,n
$$

\newpage
# Referencias
<!-- ---
nocite: |
  @*
--- -->
