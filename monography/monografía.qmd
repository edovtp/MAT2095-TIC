---
number-sections: true
highlight-style: pygments
format: 
  pdf:
    documentclass: article
    toc: true
    include-in-header: header.tex
    include-before-body: title_page.tex
    toc-title: "Tabla de Contenidos"
    toc-depth: 3
  html:
    toc: true
    code-fold: true
execute: 
  cache: true
crossref: 
  fig-title: Figura
  tbl-title: Tabla
bibliography: references.bib
jupyter: julia-1.8
---

```{julia}
#| include: false
using Random
using StatsBase
using Distributions
using ConjugatePriors
using ElasticArrays
using Plots
using StatsPlots
```

\newpage

\section*{Prefacio}

El presente material, incluyendo la monografía, códigos e ilustraciones, fue confeccionado durante mi Taller de Iniciación Científica (MAT2095), guiado por el profesor Fernando Quintana, durante el segundo semestre del 2022 y verano del 2023. Todo el material se encuentra disponible en el siguiente [repositorio](https://github.com/edovtp/MAT2095-TIC) de Github.

El objetivo principal de este taller fue implementar el algoritmo SIGN [@ni_scalable_2020], que permite aplicar modelos de Mezcla Proceso de Dirichlet (DPM) para bases de datos relativamente grandes. Lo anterior suponía un conocimiento previo de Estadística Bayesiana No Paramétrica, por lo que, en realidad, la mayor parte del trabajo se enfocó en aprender sobre esta área, especialmente sobre Procesos de Dirichlet, Mezclas de Procesos de Dirichlet y Modelos de Particiones Aleatorias, incluyendo tanto la teoría como los métodos computacionales disponibles.

En cuanto a la parte computacional, en R existen diferentes paquetes que implementan diferentes modelos Bayesianos no paramétricos, como `DPPackage` [@jara_dppackage_2011] y `dirichletprocess` [@ross_dirichletprocess_2020], así como modelos de particiones aleatorias como `salso` [@dahl_search_2022] y `ppmSuite` [@page_ppmsuite_2022]. Por otro lado, en Julia [@bezanson_julia_2017], que recientemente ha aumentado considerablemente el número de usuarios, no existe mucho desarrollo respecto a los modelos anteriores. Considerando lo anterior, todos los códigos fueron implementados en este lenguaje. Como proyecto a futuro, se podría incluso formar una librería con este material.

La monografía incluye una pequeña introducción tanto a la estadística Bayesiana como al enfoque no paramétrico, para así entender cómo se mezclan ambos conceptos. Luego, se presentan los Procesos de Dirichlet (DP), que es probablemente el punto de partida más común en la estadística Bayesiana no paramétrica. Ya entendiendo estos procesos, pasaremos a una extensión que será el enfoque principal de todo este trabajo, que son los Dirichlet Process Mixture Models (DPM), realizando una pequeña revisión histórica de los métodos de simulación desarrollados. Finalmente, veremos la conexión entre estos modelos con los Modelos de Particiones Aleatorias, para luego aplicarlo en el contexto de clustering.

\newpage
# Introducción

El material presentado a continuación se enmarca en el área de la Estadística Bayesiana No-Paramétrica, por lo que, en primer lugar, es una buena idea presentar una breve introducción de ambos conceptos. Lo siguiente no pretende ser una introducción exhaustiva, para lo cual existe bibliografía mucho más adecuada.

## Estadística Bayesiana

### Tipos de Incertidumbre

Es común que al presentarnos como estadísticos se nos pregunte acerca de qué es lo que hacemos en nuestro trabajo, ante lo cual solemos responder que nuestro objetivo principal es el de cuantificar la incertidumbre. Explicamos, además, que para lo anterior nos apoyamos sobre la teoría de probabilidades, tomándola como herramienta principal para modelar aquellas incertezas de interés.

Pero, quizás nosotros mismos como estadísticos no hemos reparado acerca de a qué nos referimos exactamente con *incertidumbre*. Esta pregunta es la que nos lleva a las bases mismas de la estadística, así como a entender cómo surgen dos visiones que son diferentes entre sí: la **Estadística frecuentista** (también denominada clásica) y la **Estadística Bayesiana**.

En particular, se distinguen dos tipos de incertidumbres [@ohagan_dicing_2004]. Una de ellas la podemos denominar **incerteza ontólogica** (o aleatoria), mientras que la otra toma el nombre de **incerteza epistemológica** [^1].

[^1]: Es importante mencionar que la ontología es el estudio filosófico del *ser*, mientras que la epistemología es el estudio filosófico del *saber*.

La incerteza ontológica trata acerca de una incerteza que está sujeta a una variabilidad aleatoria innata, que no podemos predecir bajo ninguna cantidad de información. Dentro de los ejemplos de incerteza ontológica se encuentran varios de los ejemplos introductorios a la estadística, como el lanzamiento de un dado o el de ganar la lotería.

Por otro lado, la incerteza epistemológica, tal como lo dice el nombre, es una incerteza acerca de lo que sabemos. La diferencia con la anterior es que en este caso sí podemos obtener información para disminuir, e incluso a veces eliminar, la incerteza. Por ejemplo, podemos tener incerteza acerca de la altura del Costanera Center en Santiago, pero podemos fácilmente buscar en internet esta información, eliminando completamente la incerteza [^2].

[^2]: De hecho, el edificio central tiene una altura de 300 metros.

Así, cuando hablamos de cuantificar la incertidumbre con probabilidades, podemos notar que siempre nos hemos estado refiriendo a incertezas ontológicas. En este sentido, las probabilidades se interpretan como la frecuencia de ocurrencia de un evento, considerando un número infinito de repeticiones. Este es el paradigma *frecuentista* de la estadística.

Ahora, ¿por qué no podemos modelar también las incertezas epistemológicas mediante probabilidades?. Esto es precisamente, de manera justificada, lo que propone el paradigma *Bayesiano*. En este caso ya no podemos interpretar las probabilidades como frecuencias, si no que como una *medida racional de incerteza*, lo cual normalmente dependerá de cada persona.

### Modelos Bayesianos

Como vimos, el paradigma Bayesiano se basa en la idea de probabilidad subjetiva, donde estas cantidades reflejan el grado de creencia que un individuo tiene con respecto a eventos particulares.

En cuanto al modelamiento, estas creencias son plasmadas en una distribución a priori de los parámetros de interés, $\pi(\theta)$ [^3], denominada simplemente **priori** de aquí en adelante. Además, debemos definir la verosimilitud de nuestros datos, $p(\mathbf{y}|\theta)$, que refleja justamente qué tan verosímiles son nuestros datos observados, dado un cierto valor de $\theta$. Finalmente, ambas componentes son utilizadas para definir un modelo conjunto tanto de cantidades observables como no observables, esto es,

[^3]: $\theta$ puede ser multidimensional.

$$
p(\mathbf{y}, \theta) = p(\mathbf{y}|\theta)\pi(\theta)
$$

Luego, a la luz de nueva información, se actualiza nuestra creencia a priori, mediante el teorema de Bayes, obteniendo entonces la distribución a posteriori que llamaremos simplemente **posteriori** en lo que sigue.

$$
\begin{aligned}
  \pi(\theta|\mathbf{y}) &= \frac{f(\mathbf{y}|\theta)\pi(\theta)}{\int_{\Theta}f(\mathbf{y}|\theta)\pi(\theta)d\theta} \\
  &\propto f(\mathbf{y}|\theta)\pi(\theta)
\end{aligned}
$$ {#eq-posterior}

Por otro lado, es posible que también tengamos interés en la predicción de valores observables a futuro. Esto se obtiene fácilmente marginalizando la incerteza con respecto a las cantidades no observables, i.e.

$$
\begin{aligned}
  p(y_{n+1}|\mathbf{y}) &= \int p(y_{n+1}, \theta|\mathbf{y})d\theta \\
  &=\int p(y_{n+1}|\theta)\pi(\theta|\mathbf{y})d\theta
\end{aligned}
$$ {#eq-posterior-predictive}

donde en la segunda ecuación realizamos el supuesto de independencia condicional entre las observaciones, dado los valores del vector de parámetros $\theta$. Ambos resultados, ([-@eq-posterior]) y ([-@eq-posterior-predictive]), podemos considerarlos como los productos principales para ser utilizados en la inferencia estadística mediante este paradigma.

\newpage
```{=tex}
\begin{ejemplo}[Modelo Normal-Normal]
Para aterrizar los conceptos anteriores, consideremos el siguiente modelo:
$$
\begin{aligned}
  y_1, ..., y_n | \theta \overset{i.i.d.}&{\sim} \text{N}(\theta, \sigma^2) \\
  \theta &\sim \text{N}(\mu_0, \sigma_0^2)
\end{aligned}
$$

donde $\sigma^2$ es conocido. Utilizando la ecuación (1) es fácil mostrar que

$$
  \theta | y_1, ..., y_n \sim N(\mu_n, \sigma_n^2)
$$

donde 

$$
  \mu_n = \frac{(1/\sigma_0^2)}{1/\sigma_0^2 + n/\sigma^2}\mu_0 + \frac{(n/\sigma^2)}{1/\sigma_0^2 + n/\sigma^2}\bar{y}
$$

y

$$
  \sigma_n^2 = \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}
$$

Por otro lado, también es posible mostrar que, utilizando la ecuación (2),

$$
  y_{n+1}|\mathbf{y} \sim \text{N}(\mu_n, \sigma^2 + \sigma_n^2)
$$
\end{ejemplo}
```

El ejemplo anterior muestra una de las características principales de la estadística Bayesiana, que es la combinación de la información a priori con la información de los datos en los resultados a posteriori. En particular, notamos que:

* La media a posteriori corresponde a un promedio ponderado de la media a priori y el promedio muestral.
* La varianza a posteriori es un promedio armónico de la varianza a priori y la varianza del promedio muestral.

Además, en el ejemplo anterior obtuvimos un resultado bastante conveniente. Al considerar una función de verosimilitud Normal, así como una priori Normal para $\theta$, obtuvimos que la posteriori sigue siendo una distribución Normal. Este tipo de modelos se denominan **conjugados** y serán importantes en los siguientes capítulos. Presentamos la definición formal a continuación.

\newpage
```{=tex}
\begin{definicion}[Priori conjugada]
  Decimos que una clase $\mathcal{P}$ de distribuciones a priori es \textbf{conjugada} para la verosimilitud $f(\mathbf{y}|\theta)$ si
  $$
    \pi(\theta) \in \mathcal{P} \implies \pi(\theta|\mathbf{y}) \in \mathcal{P}
  $$
  esto es, la distribución a posteriori de $\theta$ sigue teniendo la misma distribución que la priori.
\end{definicion}
```

Los modelos conjugados son convenientes ya que obtenemos resultados analíticamente tractables, siempre y cuando esta clase sea analíticamente tractable, así como generalmente intuitivos. Ahora, nada nos debe restringir a ocupar modelos conjugados. En particular, vemos que, en un principio, tanto para la verosimilitud como para la priori podemos ocupar cualquier distribución de probabilidad, que deberán ser elegidas de acorde al problema.

Lo anterior provocó uno de los cuellos de botella más importantes de la estadística Bayesiana, que frenó su amplio uso en la práctica, donde los modelos necesitaban ser cada vez más realísticos. Principalmente, el problema radica en los cálculos computacionales necesarios para realizar la inferencia a posteriori, que incluye tanto la evaluación de ([-@eq-posterior]) y ([-@eq-posterior-predictive]), las cuales no siempre pueden ser obtenidas de forma analítica.

Para resolver parcialmente lo anterior se ha recurrido principalmente a métodos de simulación, utilizando muestras a posteriori para evaluar diferentes cantidades de interés, como momentos, cuantiles, probabilidades, etc. Entre estos métodos computacionales se encuentran Rejection Sampling, Importance Sampling y métodos basados en cadenas de Markov como Slice sampling, Gibbs Sampling, Metropolis-Hastings, Hamiltonian Monte Carlo (HMC) y No-U-Turn Sampler (NUTS), Sequential Monte Carlo, entre otros. Para una introducción a cada uno de los métodos anteriores, ver [@gelman_bayesian_2014].

No solo estos modelos se han ido perfeccionando con el tiempo, si no que también se han desarrollado diferentes softwares que permiten definir nuestros modelos para realizar la simulación a posteriori, sin tener que uno programar toda la maquinaria necesaria. Algunos ejemplos son los lenguajes probabilísticos BUGS, JAGS, Stan, PyMC3 y Turing. Además, existen librerías escritas sobre estos lenguajes probabilísticos, que ayudan aún más a que personas de otras áreas puedan fácilmente ajustar sus modelos.

\newpage
## Estadística No-Paramétrica

Normalmente, en estadística asumimos

$$
y_1,..., y_n | G \overset{i.i.d.}{\sim} G
$$

Suponemos que la densidad de $G$, $g$, pertenece a 

$$
\mathcal{G} = \{g_\theta\colon \theta \in \Theta \subset \mathbb{R}^p\}
$$

* Ejemplo

* Figura

::: {#fig-np layout-ncol=2}
![](figures/NP%20-%20Example%201.jpg)

![](figures/NP%20-%20Example%202.jpg)

Necesidad de métodos flexibles
:::

* Nos gustaría ir un poco más allá: estimación de densidades (figura) y regresión

## Estadística Bayesiana No-Paramétrica

\newpage
# Procesos de Dirichlet

Consideremos, en primer lugar, el problema de estimación de densidades, lo cual en el contexto bayesiano se traduce a la estimación de la posteriori predictiva $p(y_{n+1}|\mathbf{y})$. Específicamente, consideremos una muestra condicionalmente independiente dado $G$, $y_1, ..., y_n$, donde $G$ es una medida de probabilidad no especificada, denotado por $y_1, ..., y_n | G \overset{i.i.d.}{\sim} G$.

Considerando el paradigma Bayesiano, lo anterior debe ser completado con una medida de probabilidad sobre $G$, esto es, sobre el espacio de las medidas de probabilidad. Una de estas opciones es la del Proceso de Dirichlet [@ferguson_bayesian_1973], que probablemente sea uno de los puntos de partida más usuales a los modelos no paramétricos y que tomará una gran importancia en los siguientes capítulos.

## Definición

Presentamos a continuación la definición formal de estos procesos.

```{=tex}
% Definición Proceso de Dirichlet
\begin{definicion}[Proceso de Dirichlet]
  Sea $M>0$ y $G_0$ una medida de probabilidad definida sobre $S$. Un \textbf{Proceso de Dirichlet (DP)} de parámetros $(M, G_0)$, denotado por $\text{DP}(M, G_0)$, es una medida de probabilidad aleatoria $G$ definida en $S$ que asigna probabilidad $G(B)$ a todo conjunto medible $B$ tal que, para toda partición medible finita $\{B_1, ..., B_k\}$ de $S$, la distribución conjunta del vector $(G(B_1), ..., G(B_k))$ es Dirichlet con parámetros

$$
(M G_0(B_1), ..., M G_0(B_k))
$$

Los parámetros $G_0$ y $M$ se denominan la \textbf{medida de centralización} y la \textbf{precisión}, respectivamente. También se suele denominar $M G_0$ como la \textbf{medida base}.
\end{definicion}
```

```{=tex}
\begin{nota}
  En la mayoría de la literatura se suele utilizar $\alpha$ en vez de M. Decidí en este caso utilizar la segunda opción para así tener disponible la nomenclatura $\alpha$ para parámetros de otras distribuciones, como por ejemplo la Gamma.
\end{nota}
```

En su artículo, Ferguson muestra que, bajo ciertas condiciones de regularidad, $G$ existe para todo $G_0$. Además, señala algunas propiedades de los DP, algunas de las cuales se deducen directamente de la definición del proceso al considerar la partición $\{B, B^c\}$.

\newpage
```{=tex}
\begin{propiedad}[(Propiedades Proceso de Dirichlet)]
  Sea $G \sim \text{DP}(M, G_0$), $B, B_1$ y $B_2$ conjuntos medibles, con $B_1 \cap B_2 = \emptyset$. Luego,
  \begin{enumerate}
    \item El soporte de $G$ coincide (casi seguramente) con el de $G_0$. Esto es,
    \begin{equation*}
      G_0(B) = 0 \implies P(G(B) = 0) = 1
    \end{equation*}
    y
    \begin{equation*}
      \quad G_0(B) > 0 \implies P(G(B) > 0) = 1 
    \end{equation*}
    \item $\text{E}(G(B)) = G_0(B)$
    \item $\text{Var}(G(B)) = \frac{G_0(B)(1 - G_0(B)}{1 + M}$
    \item $\text{Cov}(G(B_1), G(B_2)) = \frac{-G_0(B_1)G_0(B_2)}{1 + M}$
  \end{enumerate}
\end{propiedad}
```

La segunda propiedad nos muestra la razón por la que $G_0$ recibe el nombre de *medida de centralización*. Por otro lado, la tercera propiedad nos da la intuición por la que el parámetro M recibe el nombre de *parámetro de precisión*, al controlar el grado de concentración en la media $G_0(B)$. Por último, la cuarta propiedad nos entrega una característica bastante interesante, y es que la covarianza entre dos conjuntos cualesquiera es siempre negativa. Lo anterior puede ser una característica no deseada, donde esperaríamos que exista una correlación positiva entre conjuntos $B_1$ y $B_2$ cercanos entre sí, pero aquellas extensiones están fuera del alcance de este trabajo.

```{=tex}
\begin{nota}
  Algunos se preguntarán, como yo lo hice al empezar a estudiar estos temas, el por qué lo anterior se denomina un \textbf{proceso}. La razón es bastante sencilla, y es que el DP es un proceso estocástico que, en vez de estar indexado por índices comunes como el tiempo o coordenadas geográficas, está indexado por todos los conjuntos medibles, esto es, para cada conjunto medible $B$ tenemos la variable aleatoria $G(B)$.
\end{nota}
```

Una propiedad muy importante, que será central en el transcurso de esta monografía, es que $G$ es casi-seguramente discreta. Este resultado nos dice que $G$ se puede escribir como una suma ponderada de masas puntuales (también denominados *átomos*). Esto es,

$$
G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot)
$$

donde $\sum_{h=1}^\infty w_h = 1$ y $\delta_{x}(\cdot)$ denota la medida de Dirac en $x$, i.e. $\delta_x(A) = 1$ si $x\in A$ y $\delta_x(A) = 0$ en caso contrario. En la Figura 2 se ilustra gráficamente un Proceso de Dirichlet. A la izquierda se ilustran los átomos con puntos morados, donde los largos indican el peso que aporta cada uno. A la derecha se muestra cómo se calcularía la probabilidad para un cierto conjunto medible $B$, que es simplemente tomar la suma de las masas de los átomos que se encuentran dentro de este conjunto.

::: {#fig-dp layout-ncol=2}
![](figures/DP%20-%20Definition.jpg)

![](figures/DP%20-%20Definition%202.jpg)

Naturaleza discreta del Proceso de Dirichlet
:::

Por último, Ferguson tambien demuestra que un DP es conjugado para una muestra i.i.d. de esta distribución, donde para la medida de centralización se considera un promedio ponderado entre la medida a priori $G_0$ y la medida empírica de los datos, mientras que la precisión aumenta con el número de observaciones. En particular, tenemos el siguiente resultado.

```{=tex}
% Posteriori Proceso de Dirichlet
\begin{proposicion}[(Ferguson, 1973)]
  Sea $y_1, ..., y_n | G \overset{i.i.d}{\sim} G$ y $G \sim \text{DP}(M, G_0)$. Luego,
  \begin{equation*}
    G | y_1, ..., y_n \sim \text{DP}\left(M + n, \frac{M G_0 + n \hat{f}_n}{M + n}\right)
  \end{equation*}
  donde $\hat{f}_n$ es la medida empírica obtenida a partir de los datos, i.e.
  \begin{equation*}
    \hat{f}_n(\cdot) = \frac{1}{n}\sum_{i=1}^n \delta_{y_i}(\cdot)
  \end{equation*}
\end{proposicion}
```

Ahora, todo lo anterior aún no nos dice mucho acerca de como trabajar con esta distribución, ya que de momento sólo sabemos que existen tales procesos. En la práctica, se ha trabajado principalmente de dos formas. La primera es marginalizando la medida de probabilidad aleatoria $G$, esto es, trabajar directamente con

$$
p(y_1, ..., y_n) = \int p(y_1, ..., y_n|G) d\pi(G)
$$

La otra forma es considerar la construcción de un DP mediante una representación basada en cortar una varilla de largo unitario, de manera sucesiva e indefinida, denominada *Stick-Breaking*.

## Construcción de Procesos de Dirichlet

### Urnas de Pólya o Proceso del Restaurante Chino

Una de las formas de poder trabajar con un Proceso de Dirichlet es, irónicamente, no trabajar con él. En probabilidades esto lo logramos marginalizando con respecto a la medida de la variable que no es de interés.

Considerando una muestra aleatoria $y_1, ..., y_n|G \sim G$, Blackwell y MacQueen  [-@blackwell_ferguson_1973] formulan una representación de la densidad marginal $p(y_1, ..., y_n)$ mediante una construcción por Urnas de Pólya. En particular, sabemos que la densidad marginal conjunta podemos expresarla como

$$
p(y_1, ..., y_n) = p(y_1)\prod_{i=2}^n p(y_i|y_1, ..., y_{i-1})
$$

y lo que muestran ambos autores es que $p(y_1)$ es la densidad de $G_0$ y que, para $i = 2, ..., n$,

$$
p(y_i|y_1, ..., y_{i-1}) = \frac{1}{M + i - 1}\sum_{h=1}^{i-1}\delta_{y_h}(y_i) + \frac{M}{M + i - 1}G_0(y_i)
$${#eq-dp-marginal}

Este resultado puede ser interpretado de la siguiente manera, razón por la cuál se asocia a Urnas de Pólya: consideremos una especie de generador de colores $G_0$ y una urna en la cuál iremos depositando pelotas de diferentes colores. Partimos nuestro proceso generando un color y depositando una pelota en la urna de ese mismo color. Luego, seguimos el proceso para $i = 1, 2, ...,$ como:

* Con probabilidad $M/(M + i - 1)$ generamos un nuevo color de $G_0$ y depositamos en la urna una pelota de ese color.
* En caso contrario, sacamos una pelota al azar de la urna y devolvemos una pelota extra del mismo color.

Lo que muestran Blackwell y MacQueen es que, siguiendo este proceso de forma infinita, el proceso converge a un proceso de Dirichlet, por lo que las densidades marginales finitas coinciden y están dadas por la ecuación ([-@eq-dp-marginal]).

```{=tex}
\begin{nota}
  Esta construcción también recibe el nombre del Proceso del Restaurante Chino (CRP). En este caso, se considera un restaurante de mesas infinitas y que $G_0$ es un generador de comidas. De manera sucesiva entran clientes y los sentamos en una mesa vacía con probabilidad $M/(M + i - 1)$, sirviéndoles un nuevo plato de $G_0$ o, en caso contrario, se elige a otro cliente al azar, sentando al nuevo cliente en la misma mesa, y sirviéndole el mismo plato.
\end{nota}
```

De ([-@eq-dp-marginal]) también se pueden observar tres cosas importantes:

* Dada la intercambiabilidad de $y_1, ..., y_n$, las distribuciones condicionales completas $p(y_i|y_{-i})$ toman la misma forma que para $i = n$ en ([-@eq-dp-marginal]). Estas distribuciones son la pieza clave para realizar simulación a posteriori mediante muestreo de Gibbs.
* La distribución a priori predictiva toma la misma forma que para $i = n + 1$.
* Las condicionales ([-@eq-dp-marginal]) se pueden simplificar considerando solo los valores iguales de $y_1, ..., y_{i-1}$ (lo cual tiene probabilidad positiva dada la naturaleza discreta de $G$). Sean $y^{*}_1, ..., y^{*}_k$ los valores únicos y $n_j$, $j=1,...,k$ el número de veces que se repite cada valor. Luego, podemos escribir la distribución marginal de forma equivalente como

$$
p(y_i|y_1^*, ..., y_k^*) = \frac{1}{M + i - 1}\sum_{j=1}^{k}n_j\delta_{y_j^{*}}(y_i) + \frac{M}{M + i - 1}G_0(y_i)
$${#eq-dp-marginal-equal}

```{=tex}
\begin{ejemplo}[Urnas de Pólya]
  Para ilustrar los resultados anteriores, en la Figura 3 se presentan muestras $y_1, ..., y_n$ de tamaño 100 de la distribución marginal de un proceso de Dirichlet, considerando una medida de centralización Gamma(6, 1) y diferentes valores de $M$. \\

  Para replicar esta figura, ver el código \texttt{examples/02\_DpData.jl}.
\end{ejemplo}
```

::: {#fig-dp-urn}
![](figures/DP%20-%20Urn.png)

Muestra simulada directamente de la distribución marginal de un proceso de Dirichlet. Es posible ver que, a medida que $M$ aumenta, el histograma se va pareciendo cada vez más a $G_0$, representada en azul. 
:::

### Stick-Breaking

La construcción anterior entrega la forma de trabajar con procesos de Dirichlet de forma indirecta, por lo que es natural preguntarnos si existirá alguna opción directa. En esta línea, en los años 90 se obtiene un resultado que permite trabajar directamente con estos procesos [@sethuraman_constructive_1994]. Es importante poner énfasis en la fecha de este resultado, ya que el anterior que vimos se dio en 1973, lo cual explica por qué mucho del desarrollo en estos procesos utilizaba la construcción previa.

\newpage
En particular, se demuestra el siguiente teorema.

```{=tex}
\begin{proposicion}[(Sethuraman, 1994)]
  Sea $w_h = \upsilon_h \prod_{l<h} (1 - \upsilon_l)$ con $\upsilon_h \overset{i.i.d.}{\sim} \text{Beta}(1, M)$ y $m_h \overset{i.i.d.}{\sim} G_0$, donde $(\upsilon_h)$ y $(m_h)$ son independientes entre sí. Luego,

  \begin{equation*}
    G(\cdot) = \sum_{h=1}^\infty w_h \delta_{m_h}(\cdot) \sim \text{DP}(M, G_0)
  \end{equation*}
\end{proposicion}
```

Esta construcción recibe el nombre de \textbf{stick-breaking}, lo cual puede verse al pensar en los $\upsilon_h$ como las proporciones que se van eliminando de una varilla que inicialmente es de largo 1, mientras que los $w_h$ serían la proporción del total de la varilla que toma como peso para el átomo $m_h$. La Figura 4 presenta una pequeña ilustración de esta construcción.

Ahora, la construcción anterior sigue teniendo un pequeño problema, y es que claramente no podemos repetir el proceso de obtención de pesos y átomos de manera infinita. Una de las formas de evitar este problema es simplemente truncar la representación hasta un número $H$ fijo, fijando $\upsilon_H = 1$, denominado un \textbf{DP finito} [@ishwaran_markov_2000], u obtener los pesos $w_h$ hasta cubrir un cierto número fijo cercano a 1 de probabilidad, denominado un $\boldsymbol\epsilon$-\textbf{DP} [@muliere_approximating_1998].

::: {#fig-stickbreaking}
![](figures/DP%20-%20Stick%20Breaking.jpg)

Ilustación del proceso de Stick-Breaking. (a) Inicialmente se parte con una varilla de largo 1. (b) Se toma una proporción $\upsilon_1$ del total para el primer átomo, dándole un peso $w_1$. (c) De la varilla restante, se toma una proporción $\upsilon_2$ para el segundo átomo.
:::

```{=tex}
\begin{ejemplo}[Stick-Breaking]
  Para ilustrar esta construcción, en la Figura 5 se presentan simulaciones de procesos de Dirichlet, considerando una medida de centralización Gamma(6, 1) y diferentes valores de $M$. Específicamente, se presentan las simulaciones de $G((0, x))$ (i.e. la función de distribución acumulada).

  Para replicar esta figura, ver el código \texttt{examples/01\_Dp.jl}.
\end{ejemplo}
```

::: {#fig-dp-sb-eg}
![](figures/DP%20-%20Stick%20Breaking%20eg.png)

Muestras de un proceso de Dirichlet, representadas en gris, junto a la función de distribución acumulada de $G_0$, representada en rojo. Es posible ver cómo las simulaciones se concentran alrededor de $G_0$ y que, a medida que $M$ aumenta, la concentración es mayor, lo que concuerda con las propiedades vistas anteriormente.
:::


```{=tex}
\begin{nota}
  En este punto del trabajo fue donde decidí cambiarme de R a Julia, ya que las simulaciones anteriores tomaban demasiado tiempo. Utilizando Julia obtuve una mejoría en rapidez de casi 100 veces.
\end{nota}
```

## Otros resultados y extensiones

En las secciones anteriores vimos que, dada la naturaleza discreta del proceso de Dirichlet, existe una probabilidad positiva de repeticiones en los valores $y_1, ..., y_n$, los que denotamos por $y_1^*, ..., y_k^*$, $k \leq n$. En esta sección veremos algunos resultados con respecto a la distribución y comportamiento de $k$, el número de valores únicos en la muestra.

En primer lugar, tenemos que el comportamiento asintótico de $k$ se comporta como $M \log (n)$ cuando $n \to \infty$ [@korwar_contributions_1973] y, en particular, $k \to \infty$ casi seguramente. Por otro lado, Antoniak [-@antoniak_mixtures_1974] entrega la distribución a priori de $k$, dada por

$$
p(k|M, n) = c_n(k) n! M^k \frac{\Gamma(M)}{\Gamma(M + n)}
$$

donde $c_n(k) = P(k|M = 1, n)$. Además, muestra que:

$$
E(k|M, n) = \sum_{i=1}^n \frac{M}{M + i - 1} \approx M \left(\log (n/M + 1)\right)
$${#eq-dp-k}

Estos resultados muestran otra de las formas en que se encuentran "limitados" estos procesos. En particular, puede ser que para algún problema específico el comportamiento de $k$ no sea adecuado o, como vimos anteriormente, que la correlación entre conjuntos sea siempre negativa, independiente de la cercanía entre ellos. Nos interesan entonces generalizaciones o extensiones de los DPs.

Una de las generalizaciones son los denominados **Species Sampling Models (SSM)** [@pitman_developments_1996], con un caso especial correspondientes a las **prioris stick-breaking**, las cuales toman la misma forma que la construcción mediante stick-breaking de un DP, pero variando los parámetros de $\upsilon_h$.

De estas construcciones, probablemente la más conocida sea el **proceso de Pitman-Yor** [@pitman_two-parameter_1997], también denominado DP de 2 parámetros, el cual ya no considera los $\upsilon_h$ idénticamente distribuidos, sino que

$$
  \upsilon_h \overset{ind.}{\sim} \text{Beta}(1 - b, M + bh), \quad b \in [0, 1), \quad M > -b
$$

lo cual denotamos por $\text{PY}(M, b, G_0)$, donde el parámetro $b$ suele recibir el nombre de *parámetro de descuento*. Podemos notar que un proceso de Dirichlet es un caso especial del proceso de Pitman-Yor con $\text{PY}(M, 0, G_0) \equiv \text{DP}(M, G_0)$. En este caso, se obtiene que la representación por urnas de Pólya está dada por

$$
p(y_i|y_1, ..., y_{i-1}) = \frac{1}{M + i - 1}\sum_{j = 1}^{k_{i-1}} (n_j - b)\delta_{y_j^*}(y_i) + \frac{M + bk_{i-1}}{M + i - 1}G_0
$$

y, además, en la misma línea que Antoniak, Pitman entrega la esperanza de $k$ a priori [@pitman_combinatorial_2006] dada por

$$
E(k|M, n) = \frac{M}{b}\left(\prod_{j=1}^n \frac{M + b + j - 1}{M + j - 1} - 1\right) \approx \frac{\Gamma(M + 1)}{b\Gamma(M + b)}n^b
$${#eq-py-k}

```{=tex}
\begin{ejemplo}[Proceso de Pitman-Yor]
  En la Figura 6 se presentan muestras aleatorias de tamaño 100 de la distribución marginal de un proceso de Pitman-Yor con medida de centralización Gamma(6, 1), parámetro de descuento 0.6 y diferentes valores de M.

  Por otro lado, la Figura 7 ilustra el comportamiento de $k$ tanto para el proceso de Dirichlet y el proceso de Pitman-Yor, utilizando simulaciones. Para la aproximación de $\text{E}(k)$ se utilizaron 30 muestras para tamaños de muestra que van desde 1000 hasta 50000.

  Para replicar ambas figuras, ver los códigos \texttt{examples/01\_Py.jl} y \texttt{examples/02\_PyData.jl}.
\end{ejemplo}
```

::: {#fig-urn-py}
![](figures/PY%20-%20Urn.png)

Muestra simulada directamente de la distribución marginal de un proceso de Pitman-Yor con parámetro de descuento 0.6. En este caso vemos que el número de componentes únicos es mayor a medida que $M$ aumenta.
:::

::: {#fig-stickbreaking-py}
![](figures/PY%20DP%20comparison.png)

Estimaciones empíricas de $E(k)$ mediante simulación tanto para el proceso de Dirichlet y el proceso de Pitman-Yor. Vemos que la tasa de crecimiento de $k$ es más alta en el proceso de Pitman-Yor.
:::

\newpage
# Dirichlet Process Mixture Models

## Introducción y definición

La generación de distribuciones discretas puede no ser adecuada para algunas situaciones, por lo que entonces, con el objetivo de formular modelos que realmente reflejen el problema considerado, nos interesa extender los procesos de Dirichlet [^4].

[^4]: En lo que sigue, utilizaremos principalmente la notación de Neal (2000), mezclándola en parte con la del libro *Bayesian Nonparametric Data Analysis* [@muller_bayesian_2015].

Una opción es usar alguna medida de probabilidad aleatoria $G$ como la distribución mezclante en un modelo de mezcla, esto es, $f_G(y) = \int f_\theta(y)dG(\theta)$. En particular, utilizando una priori stick-breaking, obtenemos

$$
f_G(y) = \int f_\theta(y)dG(\theta) = \int f_\theta(y) d \sum_{h=1}^\infty w_h \delta_{m_h}(\theta) = \sum_{h=1}^\infty w_h f_{m_h}(y) 
$${#eq-dpm-mezcla}

Luego, notamos que el uso de estas distribuciones entrega una extensión natural de los modelos de mezcla finita a un número infinito de componentes. En este caso se vuelve necesario realizar una pequeña distinción, ya que pese a que consideramos un número infinito de componentes el número de datos es finito y, por lo tanto, tendremos a lo más un número finito de componentes "poblados", donde decimos que dos observaciones $y_i$, $y_j$, $i\neq j$ están en el mismo grupo o \textit{cluster} si los parámetros correspondientes, $\theta_i$ y $\theta_j$ son iguales. De aquí en adelante, cuando hablemos del número de componentes, nos referiremos exclusivamente al número de componentes poblados $k$. Notamos que el número de datos $n$ será siempre una cota superior de este número.

Volviendo al modelo anterior, nos concentraremos en el caso $G \sim \text{DP}(M, G_0)$, pero debemos recordar que en la práctica podemos utilizar cualquier medida de probabilidad aleatoria. Con esta elección, el modelo dado en ([-@eq-dpm-mezcla]) recibe el nombre de **Dirichlet Process Mixture Model (DPM)**.

```{=tex}
\begin{nota}
  Esto no fue mencionado anteriormente, pero la distribución de $w = (w_1, w_2, ...)$ en un proceso de Dirichlet, definida sobre el "simplex infinito-dimensional", se denomina la \textbf{distribución de Griffiths-Engen-McCloskey (GEM)}. Así, tenemos que si $w \sim \text{GEM}(1, M)$ y $m_1, m_2, ... \overset{i.i.d.}{\sim} G_0$, entonces $\sum w_h \delta_{m_h}(\cdot) \sim \text{DP}(M, G_0)$. 
\end{nota}
```

De manera equivalente, el modelo dado en ([-@eq-dpm-mezcla]) puede ser escrito a través de un modelo jerárquico, al introducir variables latentes $\theta_i$ para cada una de las observaciones:

$$
\begin{aligned}
y_i|\theta_i \overset{ind.}&{\sim} f_{\theta_i} \\
\theta_i|G \overset{i.i.d.}&{\sim} G \\
G &\sim \text{DP}(M, G_0)
\end{aligned}
$${#eq-dpm-simple}

De manera aún más general, podemos incluir en el modelo inferencia sobre $M$ y los hiperparámetros de la medida de centralización $G_0$, definiendo una distribución sobre este espacio conjunto, escogidas generalmente de manera independiente. Esto es,

$$
\begin{aligned}
  y_i|\theta_i \overset{ind.}&{\sim} f_{\theta_i} \\
  \theta_i | G \overset{i.i.d.}&{\sim} G \\
  G| M, \eta &\sim \text{DP}(M, G_\eta) \\
  M &\sim \pi_{M} \\
  \eta &\sim \pi_{\eta}
\end{aligned}
$${#eq-dpm}

Este último modelo será el enfoque de este capítulo, pero, con el objetivo de aterrizar los resultados, nos concentraremos inicialmente en el modelo ([-@eq-dpm-simple]).

## Estimación de densidades utilizando DPMs

Recordemos que nuestra motivación inicial para estudiar medidas aleatorias es la estimación de la densidad a posteriori predictiva $p(y_{n+1}|\mathbf{y})$, la cual podemos obtener al integrar con respecto a la variable latente $\theta_{n+1}$ correspondiente:

$$
p(y_{n+1}|\mathbf{y}) = \int f_{\theta_{n+1}}(y_{n+1})dP(\theta_{n+1}|\mathbf{y})
$$

Lo anterior puede llegar a ser muy complejo de calcular analíticamente (esto no es algo que ocurra solo con modelos no paramétricos), por lo que generalmente recurrimos al uso de métodos de simulación. En particular, si denotamos por $\theta_{n+1}^{(t)}$, $t=1, ..., T$, muestras de $\theta_{n+1}|\mathbf{y}$, tenemos que:

$$
p(y_{n+1}|\mathbf{y}) \approx (1/T)\sum_{t=1}^T f_{\theta_{n+1}^{(t)}}(y_{n+1})
$$

luego, nuestra meta será la obtención de muestras de $\theta_{n+1}$ a posteriori.

La primera opción para obtener estas muestras es a través del típico proceso lineal de simulación en modelos jerárquicos, donde primero simulamos $G$ de su distribución a posteriori, para luego simular $\theta_{n+1}$ de $\theta_{n+1}|G$ (no es necesario pasar por $\theta$ dada la independencia condicional). En esta línea, para poder simular $G$ tenemos el siguiente resultado sobre su distribución a posteriori, luego de marginalizar con respecto a $\theta = (\theta_1, ..., \theta_n)$ [@antoniak_mixtures_1974].

```{=tex}
\begin{proposicion}
  Considerando el modelo dado en (8), se tiene que
  $$
  G | y \sim \int \text{DP}(MG_0 + \sum_{i=1}^n \delta_{\theta_i})dP(\theta|\mathbf{y})
  $$
\end{proposicion}
```

Ahora, si en el capítulo anterior vimos que era difícil simular de la posteriori de $G$, este caso es aún peor, por lo cual debemos buscar una segunda opción que sí sea feasible para obtener las muestras deseadas. Como en el capítulo anterior, la idea será trabajar con la marginalización con respecto a $G$, i.e. la representación por urnas de Pólya. En este caso, el proceso de simulación consiste en:

1. Simular de $p(\theta|\mathbf{y})$
2. Simular de $p(\theta_{n+1}|\theta)$

Notamos que el segundo paso ya sabemos cómo realizarlo, que es utilizando la distribución dada en ([-@eq-dp-marginal]) con $i=n+1$ o, equivalentemente, la dada en ([-@eq-dp-marginal-equal]). Así, lo nuevo en este caso será la obtención de $p(\theta|\mathbf{y})$ junto con la correspondiente forma de simular de esta distribución.

## Simulación a posteriori

Una de las formas que tenemos de simular de la distribución deseada es utilizar algoritmos tipo MCMC, en particular el algoritmo de muestreo de Gibbs [@geman_stochastic_1984], lo cual requiere la obtención y posible simulación de las distribuciones condicionales completas $p(\theta_{i}|\theta_{-i}, \mathbf{y})$, $i = 1, ..., n$.

Para la obtención de estas distribuciones, ya sabemos que las prioris $p(\theta_i|\theta_{-i})$ vienen dadas por ([-@eq-dp-marginal]) con $i = n$. Mezclando esto con el hecho que $y_i|\theta_i \sim f_{\theta_i}$ y la independencia condicional, tenemos que (siendo un poco desprolijos con la notación):

$$
\begin{aligned}
  p(\theta_i|\theta_{-i}, \mathbf{y}) &\propto p(\mathbf{y}|\theta)p(\theta_i|\theta_{-i}) \\
  &= \prod_{j=1}^n f_{\theta_j}(y_j) \left[\frac{1}{M + n - 1} \sum_{j\neq i}\delta_{\theta_j}(\theta_i) + \frac{M}{M + n - 1}G_0(\theta_i)\right] \\
  &\propto \sum_{j\neq i}f_{\theta_i}(y_i)\delta_{\theta_j}(\theta_i) + Mf_{\theta_i}(y_i)G_0(\theta_i)
\end{aligned}
$$

Finalmente, al normalizar las expresiones anteriores, obtenemos que

$$
\theta_i | \theta_{-i}, y_i \sim \sum_{j\neq i}q_{i,j}\delta_{\theta_j} + r_i H_i
$${#eq-post1}

donde $H_i$ corresponde a la distribución a posteriori que se obtendría al considerar el modelo $y_i|\theta_i \sim f_{\theta_i}$ y $\theta_i \sim G_0$, esto es, $H_i \propto f_{\theta_i}(y_i)G_0(\theta_i)$, mientras que los pesos $q_{i, j}$ y $r_i$ están dados por 

$$
\begin{aligned}
  q_{i,j} &= bf_{\theta_j}(y_i) \\
  r_i &= b M \int f_{\theta}(y_i) dG_0(\theta)
\end{aligned}
$$

donde $b$ es la constante tal que la suma de los pesos sea 1. Notamos entonces que hay dos problemas claves: obtener muestras de la distribución $H_i$, $i = 1,...,n$, así como calcular la integral presente en $r_i$.

A continuación, veremos algunas formas de lidiar con ambos problemas, realizando una revisión histórica de los diferentes algoritmos basados en muestreo de Gibbs. Culminaremos en lo que se denomina el *Algoritmo 8* de Neal [-@neal_markov_2000], que permite realizar simulación en el modelo general dado en ([-@eq-dpm]).

### Caso conjugado

Una forma de mitigar el problema de muestrear de $H_i$ es elegir directamente $G_0$ tal que obtengamos una distribución conocida de la cual sepamos muestrear. La forma más simple de hacer esto es utilizar modelos conjugados, esto es, que $H_i \propto f_\theta G_0$ tenga la misma distribución que $G_0$. Esto suele también arreglar el problema de calcular $r_i$ ya que, generalmente, podemos obtener una expresión analítica de la integral correspondiente.

Como ejemplo, consideremos el modelo DPM con verosimilitud $f_{\theta_i}(y_i) \equiv \text{Normal}(y_i|\mu_i, V_i)$, donde denotamos $\theta_i = (\mu_i, V_i)$. En este caso, una de las opciones conjugadas es utilizar una distribución Normal-Gamma-Inversa (NIG):
$$
\begin{aligned}
  y_i | \theta_i &\sim \text{Normal}(\mu_i, V_i) \\
  \theta_i | G &\sim G \\
  G &\sim \text{DP}(M, G_0) \\
  G_0 &= \text{NIG}(m, \gamma, s/2, S/2)
\end{aligned}
$$

donde $\gamma$ corresponde al parámetro de escala, esto es, $G_0 \equiv \text{Normal}(\mu | m, \gamma V)\cdot \text{IG}(V | s/2, S/2)$. Con esta especificación, obtenemos que:

\begin{enumerate}
  \item La distribución de $H_i$ es $\text{NIG}(m', \gamma', s'/2, S'/2)$, donde
  \begin{itemize}
    \item $m' = (m + \gamma y_i)/(1 + \gamma)$
    \item $\gamma' = \gamma (1 + \gamma)$
    \item $s' = s + 1$
    \item $S' = S + (y_i - m)^2/(1 + \gamma)$
  \end{itemize}
  \item $r_i = M \text{t}(y_i|s, m, \sqrt{S/s (1 + \gamma)})$, donde denotamos por $t(y_i|s, m, \sigma)$ la densidad de la distribución t-Student con $s$ grados de libertad, media $m$ y escala $\sigma$, evaluada en $y_i$.
\end{enumerate}

```{=tex}
\begin{ejemplo}[Modelo conjugado - verosimilitud Normal]
Como aplicación del modelo anterior, la Figura 8a presenta una muestra aleatoria tamaño 50 de un modelo de mezcla finita con cuatro componentes Normales de parámetros $(-5, 1)$, $(-1, 1)$, $(0, 1)$ y $(5, 1)$, asignando pesos 0.15, 0.25, 0.3 y 0.3, respectivamente.

Las Figuras 8b, 8c y 8d presentan la estimación de la densidad a través de un DPM con parámetros $M=1$, $m=0$, $\gamma = 100$, $s = 4$ y $S=2$, un KDE utilizando un kernel normal y un modelo de mezcla finita, respectivamente, donde en el último caso el número de componentes fue elegido utilizando AIC.

En este caso, el DPM parece ser más cercano a la densidad real (aunque claramente en la práctica no la conocemos). La implementación del algoritmo y la figura se encuentran en los códigos $\texttt{DpmNorm.jl}$ y $\texttt{examples/04\_DpmNorm.jl}$, respectivamente.
\end{ejemplo}
```

::: {#fig-densityestimation-normal-py}
![](figures/DPM%20-%20density%20estimation.png){height=37%}

Estimación de densidad para datos simulados de un modelo de mezcla finita de 4 componentes.
:::

Ahora, nos podemos preguntar, ¿qué ganamos exactamente al considerar un modelo DPM en vez de modelos tradicionales?. En este sentido, probablemente el primer método que se nos viene a la mente para realizar estimación de densidades es el KDE (se podría argumentar que en realidad es el histograma), el cual considera como hiperparámetro el ancho de banda a utilizar por el kernel, usualmente elegido mediante alguna regla fija. Por otro lado, en el modelo de mezcla finita podemos ajustar diferentes modelos, cada uno con un número de componentes diferentes, eligiendo al final el "mejor modelo" considerando alguna métrica como el AIC, BIC o lppd.

El modelo DPM ofrece tres mejoras, una con respecto a KDE y dos con respecto a un modelo de mezcla finita. Con respecto a KDE, podemos considerar el DPM como una especie de generalización donde no se utiliza un ancho de banda fijo para todo el rango de valores. Con respecto al modelo de mezcla finita las ganancias son un poco más interesantes.

En primer lugar, una vez ajustado el modelo finito el número de componentes queda completamente fijo, mientras que en el DPM siempre dejamos la oportunidad que una nueva observación provenga de una nueva componente aún no poblada, con lo cual obtenemos un modelo que ofrece mayor flexibilidad. Por otro lado, en vez de tener que elegir el número de componentes a través de alguna regla, podemos realizar directamente inferencia **probabilística** con respecto a este parámetro $k$. Como ejemplo, la Figura 9 presenta un histograma con los valores de $k$ para cada una de las muestras de la simulación anterior.

::: {#fig-k-inference}
![](figures/DPM%20-%20inference%20on%20k.png)

Inferencia sobre el número de componentes. El número real de componentes está representado en rojo, mientras que la recta negra ilustra el promedio del número de componentes obtenidos por la simulación.
:::

A partir de la Figura 9 podemos notar que nuestro modelo tiende a sobreestimar el número real de componentes. Este comportamiento parece venir del hecho de considerar el parámetro $M$ fijo, donde debemos recordar que este parámetro, junto con el número de observaciones, influye fuertemente en el número de clusters a priori. En particular, si es de nuestro interés realizar inferencia sobre el número de componentes, entonces será deseable introducir este parámetro al modelo, lo cual nos motiva al modelo completo dado en ([-@eq-dpm-mezcla]).

### Problema de sticky-clusters

Antes de integrar al algoritmo la inferencia sobre $M$ y $\eta$, introduciremos un poco de notación motivada por un problema de eficiencia que presenta el algoritmo anterior. Este problema es conocido como *sticky clusters* y se debe a que las probabilidades de transición permiten con una probabilidad muy baja que los parámetros $\theta_i$ cambien a través de las iteraciones, sobre todo cuando los clusters de observaciones son demasiado grandes, lo cual provoca una lenta convergencia a la distribución a posteriori deseada. En la Figura 10 se ilustra este problema, donde se muestran las últimas 250 simulaciones de $\mu_1|\theta_{-1}, \mathbf{y}$ en el ejemplo anterior.

::: {#fig-sticky-clusters}
![](figures/DPM%20-%20sticky%20clusters.png)

Ilustración del problema de sticky clusters. Vemos que a través de las iteraciones el valor de $\mu_1$ se mantiene en el mismo valor a lo largo de varias ventanas de tiempo.
:::

Intuitivamente, este problema ocurre ya que el valor de $\theta$ de cada observación contiene dos piezas de información: el cluster al que pertenece la observación, lo cual se mide a partir de la igualdad entre parámetros, y el valor mismo del parámetro, los cuales se actualizan de forma conjunta en cada iteración.

La solución a lo anterior es relativamente simple, y es separar estos dos pedazos de información de alguna manera, lo cual fue el enfoque de Bush y MacEachern [-@bush_semiparametric_1996]. Para esto, denotemos por $c_i$, $i = 1,...,n$, el cluster al que pertenece cada observación, esto es $c_i = c_j$ si $\theta_i = \theta_j$, con la convención que $c_1 = 1$ y los nuevos valores sucesivos que ocurran en $\mathbf{c}$ no pueden saltarse la numeración (por ejemplo, no puede ocurrir que $c_i = 3$ si $c_j \neq 2$ para todo $j < i$). Con esta notación, notamos que existe una relación uno-a-uno entre el vector de parámetros $\theta$ y el vector de parámetros $\mathbf{c} = (c_1, ..., c_n)$ en conjunto con $\theta_1^*, ..., \theta_k^*$, lo cual ilustramos en la Figura 11.

::: {#fig-theta-separated}
![](figures/DPM%20-%20Separaci%C3%B3n%201.jpg)

Ilustración de la relación uno a uno entre el vector de parámetros latentes $(\theta_1, ..., \theta_n)$ y $(c_1, ..., c_n, \theta_1^*, ..., \theta_k^*)$. En este caso se tiene $n=8$ y $k=3$.
:::

Con esta separación de la información, tenemos que:
$$
p(\theta|\mathbf{y}) = p(\theta^*, \mathbf{c}|\mathbf{y}) = p(\theta^*|\mathbf{c}, \mathbf{y})p(\mathbf{c}|\mathbf{y})
$$

Con esto, el paso de simular de $\theta|\mathbf{y}$ podemos dividirlo en dos: simular primero de $\mathbf{c}|\mathbf{y}$ y luego de $\theta^*|\mathbf{c}, \mathbf{y}$. Para la segunda distribución, denotando por $\mathcal{C}_i = \{j\colon c_j = i\}$, tenemos que:

$$
\begin{aligned}
  p(\theta^*|\mathbf{c}, \mathbf{y}) &\propto p(\mathbf{y}|\theta)p(\theta^*|\mathbf{c}) \\
  &= \left[\prod_{i=1}^k \prod_{j \in \mathcal{C}_i} f_{\theta_i^*}(y_j) \right] \prod_{i=1}^k G_\eta(\theta_i^*) \\
  &= \prod_{i=1}^k \left[G_\eta(\theta_i^*) \prod_{j \in \mathcal{C}_i} f_{\theta_i^*}(y_j)\right]
\end{aligned}
$$

Donde la segunda distribución en el producto se obtiene al notar de ([-@eq-dp-marginal]) que un nuevo valor único siempre viene de $G_\eta$. Luego, tenemos que dados $\mathbf{c}$ e $\mathbf{y}$, cada uno de los valores únicos $\theta_i^*$, $i = 1,...,k$, son independientes, obteniendo finalmente que:

$$
p(\theta_i^*|\mathbf{c}, \mathbf{y}) \propto G_\eta(\theta_i^*)\prod_{j \in \mathcal{C}_i} f_{\theta_i^*}(y_j)
$$

Con esta información, podemos simular de la distribución deseada muestreando uno por uno $p(\theta_i^*|\mathbf{c}, \mathbf{y})$, $i = 1, ..., k$.

Para simular de $\mathbf{c}|\mathbf{y}$ utilizaremos nuevamente el algoritmo de Gibbs, por lo que necesitamos obtener las distribuciones condicionales completas $c_i|c_{-i}, \mathbf{y}$. Estas distribuciones se pueden obtener directamente de ([-@eq-dpm-simple]) al integrar con respecto a $\theta$. Denotando por $k_{-i}$ el número de valores únicos de $\theta_{-i}$, por $\mathcal{C}_{-i, j} = \{l \colon c_l = j, l \neq i\}$ y $n_{-i, j}$ el número de observaciones en este conjunto, obtenemos que:

$$
p(c_i = j| c_{-i}, \mathbf{y}) \propto \begin{cases}
  n_{-i, j} \int f_\theta(y_i) dH_{-i, j}(\theta) & j = 1, ..., k_{-i} \\
  M\int f_\theta(y_i) dG_\eta(\theta) & j = k_{-i} + 1
\end{cases}
$$

donde $H_{-i, j}(\theta) \propto G_\eta(\theta) \prod_{l \in \mathcal{C}_{-i, j}} f_{\theta}(y_l)$ y el caso $j = k_{-i} + 1$ es que el que corresponde a que la observación pertenezca a un cluster que previamente estaba vacío.

### Hiperparámetros aleatorios

Agreguemos ahora la inferencia tanto para $M$ como para $\eta$, esto es, obtener muestras de $M$ y $\eta$ a posteriori. Para esto, nos apoyaremos de algunas distribuciones conocidas y los valores muestreados de $\theta^*$ y $\mathbf{c}$.

En primer lugar, para simular de $\eta$, tenemos que dado $\theta^*$ este parámetro es independiente de $\mathbf{y}$. Luego $p(\eta|\mathbf{y}) = p(\eta|\theta^*)p(\theta^*|\mathbf{y})$. En particular,

$$
\begin{aligned}
p(\eta|\theta^*) &\propto p(\theta^*|\eta)\pi_\eta(\eta) \\
&= \left[\prod_{j=1}^k G_\eta(\theta_j^*) \right] \pi_\eta(\eta)
\end{aligned}
$$

por lo que si deseamos muestrear fácilmente de $\eta$ sería ideal elegir $\pi_\eta$ conjugado para $G_\eta$.

Con respecto a $M$, nos apoyaremos de los resultados de Antoniak vistos anteriormente con respecto a la distribución a priori $p(k|M)$. En particular, $M$ es independiente de $\mathbf{y}$ dado el valor de $k$. Luego, $p(M|\mathbf{y}) = p(M|k)p(k|\mathbf{y})$ y:

$$
\begin{aligned}
  p(M|k) &\propto \pi_M(M)p(k|M) \\
  &\propto \pi_M(M) M^k \frac{\Gamma(M)}{\Gamma(M + n)} \\
  &= \pi_M(M) M^k \frac{(M + n)}{M\Gamma(n)}\text{B}(M + 1, n) \\
  &\propto \pi_M(M) M^{k-1} (M + n)\text{B}(M + 1, n)
\end{aligned}
$$

donde en el penúltimo paso utilizamos el hecho que $\text{B}(x + 1, y) = \text{B}(x, y) x / (x + y)$. A partir de lo anterior, Escobar y West divisan que esta expresión corresponde a una distribución marginal de una distribución conjunta entre $M$ y una variable $\phi$ auxiliar:

$$
p(M|k) \propto \pi_M(M) M^{k-1} (M + n) \int \phi^M (1 - \phi)^{n-1} d\phi
$$

Luego, introducen esta variable auxiliar al algoritmo utilizando las distribuciones $p(\phi|M, k)$ y $p(M|\phi, k)$. La primera de estas está dada por:

$$
p(\phi|M, k) \propto \phi^M (1 - \phi)^{n-1} \sim \text{Beta}(M + 1, n)
$$

Por otro lado, tenemos que:

$$
\begin{aligned}
  p(M|\phi, k) &\propto \pi_M(M) M^{k-1}(M + n) \phi^M \\
    &\propto \pi_M(M) (M^k + M^{k-1}n)\exp\{M\log(\phi)\}
\end{aligned}
$$

de lo cual notamos una similitud con la función de densidad de una distribución Gamma, lo cual nos motiva a utilizar $M \sim \text{Gamma}(\alpha, \beta)$. Así:

$$
\begin{aligned}
  p(M|\phi, k) &\propto M^{\alpha - 1} \exp\{-\beta M\} (M^{k} + M^{k-1}n)\exp\{M\log(\phi)\} \\
  &\propto M^{\alpha + k - 1}\exp\{-(\beta - \log(\phi))M\} + nM^{\alpha + k - 2} \exp\{-(\beta - \log(\phi))M\} \\
  &\sim \pi_\phi \text{Gamma}(\alpha + k, \beta - \log(\phi)) + (1 - \pi_\phi) \text{Gamma}(\alpha + k - 1, \beta - \log(\phi))
\end{aligned}
$$

donde

$$
\pi_\phi = \frac{\alpha + k - 1}{n(\beta - \log\phi) + \alpha + k - 1}
$$

Finalmente, ya tenemos todo lo necesario para poder realizar inferencia a posteriori incluyendo tanto $M$ como $\eta$ en el algoritmo.

```{=tex}
\begin{nota}
  En toda esta sección se han introducido bastantes notaciones diferentes que pueden marear bastante, como me pasó a mi. En el Apéndice 1 se resumen los dos algoritmos anteriores, junto con un diccionario de notaciones.
\end{nota}
```

Como ejemplo de lo anterior, extendamos el modelo Normal visto anteriormente con los hiperparámetros $M$ y $\eta$, así como la separación entre $\theta^*$ y $\mathbf{c}$. Así, tenemos el siguiente modelo:

$$
\begin{aligned}
  y_i | \theta_i &\sim \text{Normal}(\mu_i, V_i) \\
  \theta_i | G &\sim G \\
  G &\sim \text{DP}(M, G_\eta) \\
  G_\eta &= \text{NIG}(m, \gamma, s/2, S/2) \\
  m, \gamma &\sim \text{Normal}(a, A)\text{IG}(w/2, W/2) \\
  M &\sim \text{Gamma}(\alpha, \beta)
\end{aligned}
$$

donde para simplificar las cosas, solo consideramos $\eta = (m, \gamma)$, con una priori conjunta independiente. Obtenemos entonces que:

\begin{itemize}
  \item Para $\eta|\theta^*$ tenemos que:
  \begin{enumerate}
    \item $m|\gamma, \theta^* \sim \text{Normal}\left((1 - x)a + x\bar{V}\sum_{j=1}^k \mu_j^*/V_j^*; x\gamma\bar{V}\right)$
    \item $\gamma|m, \theta^* \sim \text{IG}\left((w + k)/2, (W + K)/2\right)$
  \end{enumerate}
  donde
  \begin{enumerate}
    \item $x = A/(A + \gamma \bar{V})$
    \item $\bar{V} = \left(\sum_{j=1}^k 1/V_j^*\right)^{-1}$
    \item $K = \sum_{j=1}^k (\mu_j^* - m)^2/V_j^*$
  \end{enumerate}
  \item Para $p(\theta^*_j|\mathbf{c}, \mathbf{y})$ tenemos que
  $$
    p(\theta^*_j|\mathbf{c}, \mathbf{y}) \propto \text{NIG}(m_j^*, \gamma_j^*, s_j^*/2, S_j^*/2)
  $$
  donde
  \begin{enumerate}
    \item $m_j^* = (m + \gamma \sum_{i \in \mathcal{C}_j y_i})/(1 + \gamma n_j)$
    \item $\gamma_j^* = \gamma/(1 + \gamma)$
    \item $s_j^* = s + n_j$
    \item $S_j^* = S + \frac{n_j}{1 + \gamma n_j}(\bar{y}_{\mathcal{C}_j} - m)^2 + \sum_{i \in \mathcal{C}_j} (y_i - \bar{y}_{\mathcal{C}_j})^2$
  \end{enumerate}
  \item La distribución $H_{-i, j}$ es idéntica a la anterior pero considerando $n_{-i, j}$ y $\mathcal{C}_{-i, j}$, obteniendo entonces que:
  \begin{enumerate}
    \item $\int f_\theta(y_i)dH_{-i, j} = t\left(y_i|s_{-i, j}^*, m_{-i, j}^*, \sqrt{\frac{S_{-i, j}^*}{s_{-i, j}^*}(1 + \gamma^*)}\right)$
    \item $\int f_\theta(y_i)dG_\eta = t\left(y_i|s, m, \sqrt{\frac{S}{s}(1 + \gamma)}\right)$
  \end{enumerate}
\end{itemize}

```{=tex}
\begin{ejemplo}[Modelo conjugado completo - datos de galaxias]
Como aplicación del modelo anterior, la Figura 8a presenta una muestra aleatoria tamaño 50 de un modelo de mezcla finita con cuatro componentes Normales de parámetros $(-5, 1)$, $(-1, 1)$, $(0, 1)$ y $(5, 1)$, asignando pesos 0.15, 0.25, 0.3 y 0.3, respectivamente.

Las Figuras 8b, 8c y 8d presentan la estimación de la densidad a través de un DPM con parámetros $M=1$, $m=0$, $\gamma = 100$, $s = 4$ y $S=2$, un KDE utilizando un kernel normal y un modelo de mezcla finita, respectivamente, donde en el último caso el número de componentes fue elegido utilizando AIC.

En este caso, el DPM parece ser más cercano a la densidad real (aunque claramente en la práctica no la conocemos). La implementación del algoritmo y la figura se encuentran en los códigos $\texttt{DpmNorm.jl}$ y $\texttt{examples/04\_DpmNorm.jl}$, respectivamente.
\end{ejemplo}
```


Antes de pasar al caso no conjugado presentaremos una generalización del modelo con verosimilitud Normal a observaciones multidimensionales, el cual utilizaremos en el capítulo de clustering. En este caso, utilizamos una verosimilitud Normal multivariada, que denotaremos por $N_p(\mu_i, \Sigma_i)$, donde $\mu_i$ es el vector p-dimensional de medias y $\Sigma_i$ la matriz de covarianzas. Para esta verosimilitud, una distribución conjugada es la Normal-Wishart-Inversa (NIW):

$$
\begin{aligned}
\mathbf{y}_i | \theta_i &\sim \text{N}_p(\mu_i, \Sigma_i) \\
\theta_1, ..., \theta_n | G &\sim G \\
G|M, \eta &\sim \text{DP}(M, G_\eta) \\
G_\eta &= \text{NIW}(\textbf{m}, \gamma, \Psi, \nu) \\
M &\sim \text{Gamma}(a, b) \\
\eta &\sim \pi_\eta
\end{aligned}
$$

donde, al igual que en el caso univariado, $\gamma$ es un parámetro de escala. Además, la parametrización de la distribución Wishart-Inversa es tal que $\text{E}(\Sigma) = \Psi/(\nu - p - 1)$. Luego, tenemos que:

\begin{enumerate}
  \item La distribución de $H_i$ es $\text{NIW}(\textbf{m}', \gamma', \Psi', \nu')$, donde
  \begin{itemize}
    \item $\textbf{m}' = (\textbf{m} + \gamma \mathbf{y}_i)/(1 + \gamma)$
    \item $\gamma' = \gamma (1 + \gamma)$
    \item $\Psi' = \Psi + (\mathbf{y}_i - \mathbf{m})(\mathbf{y}_i - m)^T/(1 + \gamma)$
    \item $\nu' = \nu + 1$
  \end{itemize}
  \item $r_i = M t_p(\mathbf{y}_i|\nu - p + 1, \mathbf{m}, \sqrt{\Psi(1 + \gamma)/(\nu - p + 1)})$, donde denotamos por $t_p(\mathbf{y}_i|s, \mathbf{m}, \Sigma)$ la densidad de la distribución t multivariada con $s$ grados de libertad, media $m$ y escala $\Sigma$, evaluada en $\mathbf{y}_i$.
\end{enumerate}


### Caso no conjugado


## Otros métodos de simulación a posteriori


\newpage
# Modelos de Particiones Aleatorias y Clustering

\newpage
# Aplicación: Modelo CAPM

* El modelo de valorización de activos financieros (CAPM) fue desarrollado en los años 60 de forma independiente por Jack Treynor, William Sharpe, John Linter y Jan Mossin.

* El modelo propone 

$$
\text{E}(R) = r_f + \beta(\text{E}(R_m) - r_f)
$$

donde R es el retorno del activo, $r_f$ es la tasa de retorno libre de riesgo, $R_m$ es el retorno del mercado y $\beta$ es el riesgo sistemático del activo bajo estudio.

* Normalmente se considera el modelo de regresión

$$
Y_j \equiv r_j - r_{fj} = \alpha + \beta(r_{mj} - r_{fj}) + \varepsilon_j, \quad j = 1,...,n
$$

\newpage
# Apéndice

## Resumen algoritmos DPM

Para apoyar en la lectura, en esta sección se presentarán los tres algoritmos MCMC vistos de manera resumida, paso por paso.

### Algoritmo 1 - Estándar

### Algoritmo 2 - Sticky-Clusters e hiperparámetros

### Algoritmo 3 - Caso no conjugado

\newpage
# Referencias
<!-- ---
nocite: |
  @*
--- -->
